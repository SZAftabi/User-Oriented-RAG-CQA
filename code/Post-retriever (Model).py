# -*- coding: utf-8 -*-
"""Llama-2 (Recognizing Question Entailment)

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19Xe2U_4tlo6Z2ni-Rfjzwbm_r8ap2nOq
"""

class OverrideEpochStepCallback(Callback):
    def __init__(self) -> None:
        super().__init__()
        self.every_n_step = 1

    def on_train_epoch_end(self, trainer: pl.Trainer, pl_module: pl.LightningModule):
        self._log_step_as_current_epoch(trainer, pl_module)

    def on_test_epoch_end(self, trainer: pl.Trainer, pl_module: pl.LightningModule):
        self._log_step_as_current_epoch(trainer, pl_module)

    def _log_step_as_current_epoch(self, trainer: pl.Trainer, pl_module: pl.LightningModule):
        pl_module.log("step", trainer.current_epoch + 1)

checkpoint_callback = ModelCheckpoint(every_n_epochs = script_args.every_n_epochs,)



class RQEModel(pl.LightningModule):
    def __init__(self, script_args):
        super(RQEModel, self).__init__()
        self.save_hyperparameters()
        self.Setup(script_args)
        self.rouge = ROUGEScore()
        self.adapter_name = script_args.adapter_name
        self.epoch_n = 1
        self.validation_losses = []
        self.acc = script_args.gradient_accumulation_steps


    def Setup(self, script_args):
        if script_args.load_in_4bit and script_args.load_in_8bit:
          raise ValueError("You can't load the model in 8 bits and 4 bits at the same time")
        elif script_args.load_in_4bit:
          compute_dtype = getattr(torch, script_args.bnb_4bit_compute_dtype)

          bnb_config = BitsAndBytesConfig(
              load_in_4bit = script_args.load_in_4bit,
              bnb_4bit_quant_type = script_args.bnb_4bit_quant_type,
              bnb_4bit_compute_dtype = compute_dtype,
              bnb_4bit_use_double_quant = script_args.use_nested_quant,
          )
          self.model = AutoModelForCausalLM.from_pretrained(
              script_args.model_name,
              quantization_config = bnb_config,
              device_map = {"": 0},
          )
        elif script_args.load_in_8bit:
          self.model = AutoModelForCausalLM.from_pretrained(
              script_args.model_name,
              load_in_8bit = True,
              torch_dtype = torch.float16,
              device_map = {"": 0},
          )
          self.model = prepare_model_for_kbit_training(self.model)

        else:
          self.model = AutoModelForCausalLM.from_pretrained(
              script_args.model_name,
              torch_dtype = torch.bfloat16,
              device_map = {"": 0},
          )

        if script_args.use_peft:
            lora_config = LoraConfig(
                task_type = TaskType.CAUSAL_LM,
                r = script_args.lora_r,
                lora_alpha = script_args.lora_alpha,
                lora_dropout = script_args.lora_dropout,
                bias = "none",
                init_lora_weights = "pissa",
            )
            self.model = get_peft_model(self.model, lora_config)
            self.model.print_trainable_parameters()

        self.model.config.use_cache = False

        self.tokenizer = AutoTokenizer.from_pretrained(
            script_args.model_name,
            padding_side='left'
        )
        self.tokenizer.pad_token_id = 0
        self.model.config.pad_token_id = self.tokenizer.pad_token_id


    def forward(self, input_ids, attention_mask, labels=None):
        output = self.model(input_ids,
                            attention_mask=attention_mask,
                            labels=labels
                            )
        return output.loss, output.logits


    def training_step(self, batch, batch_idx):
        input_ids = batch['input_ids'].cuda()
        attention_mask = batch['attention_mask'].cuda()
        labels = batch['labels'].cuda()
        loss, _ = self.forward(input_ids, attention_mask, labels)
        optimizer = self.trainer.optimizers[0]
        current_lr = optimizer.param_groups[0]['lr']
        self.log('train_loss', loss.item(), on_epoch=True, on_step=True)
        self.log('learning_rate', current_lr, on_step=True, on_epoch=True)
        return loss


    def validation_step(self, batch, batch_idx):
      with torch.no_grad():
        input_ids = batch['input_ids'].cuda()
        attention_mask = batch['attention_mask'].cuda()
        labels = batch['labels'].cuda()
        val_loss, _ = self.forward(input_ids, attention_mask, labels)
        self.log('val_loss', val_loss.item(), on_epoch=True, on_step=True)


    def on_train_batch_end(self, outputs, batch, batch_idx):
      if (self.epoch_n % 1000 == 0):
          out_dir = f"{Drive_path}/RQE/RQE-Adapters"
          self.model.save_pretrained(out_dir + self.adapter_name + str(int(self.epoch_n)))
      self.epoch_n += 1


    def on_validation_epoch_end(self):
        if self.validation_losses:
            avg_val_loss = sum(self.validation_losses) / len(self.validation_losses)
            self.log("val_loss_step", avg_val_loss)
            self.validation_losses = []


    def generate(self, *args, **kwargs):
      return self.model.generate(*args, **kwargs)


    def configure_optimizers(self):
        optimizer = torch.optim.AdamW(
            self.model.parameters(),
            lr=script_args.learning_rate,
            weight_decay=0.2)
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
            optimizer,
            T_max=250 *script_args.max_epochs,
            eta_min=1e-5)
        return {
            "optimizer": optimizer,
            "lr_scheduler": {
                "scheduler": scheduler,
                "interval": "step",
                "frequency": 1}
               }


    def val_dataloader(self):
        return self.trainer.datamodule.val_dataloader()
# -*- coding: utf-8 -*-
"""Main (System).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UBGlp0usbUaQtj0ZTi2oBD1L7FzlLjgP
"""

#---------------------------------
#            Indexer
#---------------------------------

embeddings = generate_embeddings(MyData['QuestionBody'].tolist(), batch_size=batch_size)
torch.save(embeddings, "PATH/TO/YOUR/DATA.pt")

#---------------------------------
#           Retriever
#---------------------------------

k3 = 50
retrival_results = []
for i, row in TestData.iterrows():
    top_k3_indices, cosine_sim, similar_questions = get_top_k3_similar_questions(row, embeddings, MyData['QuestionBody'], k3)
    retrival_results.append({
        'RQE Question': row['body_Q1'],
        'RQE Answer': row['AnswerBody'],
        'RQE Forum': row['forum_x'],
        'Cosine similarities': cosine_sim,
        'Top n Similar Questions Body': similar_questions['QuestionBody'].tolist(),
        'Top n Candidate Answers': similar_questions['AnswerBody'].tolist(),
        'Top n Similar Questions (Cosine)': top_k3_indices,
        'Top n Similar Questions (id)': similar_questions['QuestionID'].tolist(),
        'Retrieval Time (seconds)': retrieval_time
    })
retrival_results['Top n Similar Questions Body'] = retrival_results['Top n Similar Questions Body'].apply(clean_text_list)

#---------------------------------
#       Post-retrieval
#---------------------------------

similar_questions = [item for sublist in retrival_results['Top n Similar Questions Body'].values for item in sublist[:k3]]
similar_answers = [item for sublist in retrival_results['Top n Candidate Answers'].values for item in sublist[:k3]]
CosineScores = [item for sublist in retrival_results['Cosine similarities'].values for item in sublist[:k3]]
q2_ids = [item for sublist in retrival_results['Top n Similar Questions (id)'].values for item in sublist[:k3]]

TestData = TestData.loc[TestData.index.repeat(k3)].reset_index(drop=True)
TestData['q2'] = similar_questions
TestData['CandidateAnswerBody'] = similar_answers
TestData['entailment'] = ''
TestData['CosineSimilarities'] = CosineScores
TestData['CandidateQuestionID'] = q2_ids
TestData2 = TestData.merge(
    Qs,
    left_on=['CandidateQuestionID', 'forum_y'],
    right_on=['QuestionID', 'Dataset'],
    how='left',
    suffixes=('', '_q2'))
TestData2 = TestData2[TestData.columns.tolist() + ['viewcount', 'score', 'tags_Q2', 'favoritecount']]
TestData2 = TestData2[['q1', 'q2', 'entailment', 'U_Background_kn']]

TestData2["q1"] = TestData2["q1"].str.replace(r"^\s*Possible Duplicates?:\s+.*?\s{2,}.*?\s{2,}", "", regex=True)
TestData2["q2"] = TestData2["q2"].str.replace(r"^\s*Possible Duplicates?:\s+.*?\s{2,}.*?\s{2,}", "", regex=True)
TestData2["q1"] = TestData2["q1"].str.replace(r"^\s*Possible Duplicates?:\s+.*?\s{2,}", "", regex=True)
TestData2["q2"] = TestData2["q2"].str.replace(r"^\s*Possible Duplicates?:\s+.*?\s{2,}", "", regex=True)
TestData2["q1"] = TestData2["q1"].str.replace("C#", "C", regex=False)
TestData2["q1"] = TestData2["q1"].str.replace(r"#", "", regex=True)
TestData2["q1"] = TestData2["q1"].str.replace(r"\n", " ", regex=True)
TestData2["q2"] = TestData2["q2"].str.replace("C#", "C", regex=False)
TestData2["q2"] = TestData2["q2"].str.replace(r"#", "", regex=True)
TestData2["q2"] = TestData2["q2"].str.replace(r"\n", " ", regex=True)

BaseModel= AutoModelForCausalLM.from_pretrained(
    f"{MyDrive}llama-2-7b-chat-hf",
    device_map={"": 0},
    offload_folder="offload",
    offload_state_dict = True,
    )
tokenizer = AutoTokenizer.from_pretrained(
    f"{MyDrive}llama-2-7b-chat-hf",
    padding_side='left'
    )
tokenizer.pad_token_id = 0
config = PeftConfig.from_pretrained("PATH/TO/RQE/ADAPTER")
fModel= PeftModel.from_pretrained(BaseModel, address, device_map={"": 0})
fModel = fModel.merge_and_unload()
fModel.config.pad_token_id = tokenizer.pad_token_id
fModel.config.mask_token_id = tokenizer.mask_token_id
fModel.eval()

Data_RQE = RQEDataModule(TestData2, tokenizer, script_args) # script_args: Post-retriever HyperParameters
results = []
with torch.no_grad():
    for batch in Data_RQE.test_dataloader():
        input_ids = batch['input_ids'].cuda()
        attention_mask = batch['attention_mask'].cuda()
        generated_txts_ids = fModel.generate(
            input_ids=input_ids,
            attention_mask=attention_mask,
            max_new_tokens=script_args.max_new_tokens,
            do_sample=False,
            temperature=0.0000001,
        )
        for i in range(input_ids.size(0)):
            single_generated_ids = generated_txts_ids[i]
            response_start_idx = get_response_index(single_generated_ids, 'RQE')
            single_generated_txt = tokenizer.decode(
                single_generated_ids[response_start_idx:],
                skip_special_tokens=True,
                clean_up_tokenization_spaces=True
            )
            results.append(single_generated_txt)
pd.DataFrame(results, columns = ['predicted_label']).to_pickle("PATH/TO/YOUR/DATA")

#---------------------------------
#       Generator
#---------------------------------
k4 = 3
TestData2 = TestData2[TestData2['predicted_label'] == 'positive']
TestData2 = TestData2.groupby(['id_Q1', 'forum_x']).head(k4)
TestData2["body_Q1"] = TestData2["body_Q1"].str.replace(r"\n", " ", regex=True)

for name, group in TestData2.groupby(['id_Q1', 'forum_x']):
    candidate_answers = group['CandidateAnswerBody'].tolist()
    query = group['body_Q1'].iloc[0]
    generated_answer = generate_prompt(candidate_answers, query)
    e = f
    file1_path = f"PATH/TO/GENERATOR/PROMPTS.txt"
    with open(file1_path, "a") as f1:
      f1.write(generated_answer)
    f1.close()
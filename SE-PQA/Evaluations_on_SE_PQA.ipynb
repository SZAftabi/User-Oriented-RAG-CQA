{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SZAftabi/User-Oriented-RAG-CQA/blob/main/SE-PQA/Evaluations_on_SE_PQA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z3UcuEAOGlk-"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5qqj-gvorh9"
      },
      "source": [
        "# Data Preparation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BAKuG4WeKaKT"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "crFnxwo7KEfB"
      },
      "outputs": [],
      "source": [
        "PostLinks = pd.read_csv('/content/drive/MyDrive/SE-PQA/postlinks.csv')\n",
        "PostLinks.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lBlVM3odo43-"
      },
      "outputs": [],
      "source": [
        "DuplicatePairIDs = PostLinks[PostLinks['LinkType']=='duplicated']\n",
        "display(DuplicatePairIDs.head(3))\n",
        "print(\"Total number of duplicate pairs = \", len(DuplicatePairIDs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ETgJgnVQrWaM"
      },
      "outputs": [],
      "source": [
        "columns = ['ParentId', 'Id', 'Text']\n",
        "Answers = pd.read_csv('/content/drive/MyDrive/SE-PQA/answers.csv', usecols=columns)\n",
        "Answers.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_h2tI5Peu5Z8"
      },
      "outputs": [],
      "source": [
        "Pairs_with_AcceptedAnswers = DuplicatePairIDs[\n",
        "    DuplicatePairIDs['PostId'].isin(Answers['ParentId']) &\n",
        "    DuplicatePairIDs['RelatedPostId'].isin(Answers['ParentId'])\n",
        "]\n",
        "len(Pairs_with_AcceptedAnswers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vcjj6snuvK0s"
      },
      "outputs": [],
      "source": [
        "Pairs_with_AcceptedAnswers = Pairs_with_AcceptedAnswers.merge(\n",
        "    Answers, left_on='PostId', right_on='ParentId', suffixes=('', '_Post')\n",
        ")\n",
        "display(Pairs_with_AcceptedAnswers.head(3))\n",
        "Pairs_with_AcceptedAnswers = Pairs_with_AcceptedAnswers[['PostId', 'RelatedPostId', 'LinkType', 'Id', 'Text']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r4Dx-vjmwdwH"
      },
      "outputs": [],
      "source": [
        "Pairs_with_AcceptedAnswers = Pairs_with_AcceptedAnswers.merge(\n",
        "    Answers, left_on='RelatedPostId', right_on='ParentId', suffixes=('_Post', '_Related')\n",
        ")\n",
        "Pairs_with_AcceptedAnswers = Pairs_with_AcceptedAnswers[['PostId', 'RelatedPostId', 'LinkType', 'Id_Post', 'Text_Post', 'Id_Related', 'Text_Related']]\n",
        "Pairs_with_AcceptedAnswers.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HQ_4v86ex7mt"
      },
      "outputs": [],
      "source": [
        "Pairs_with_AcceptedAnswers = Pairs_with_AcceptedAnswers.rename(\n",
        "    columns={'PostId': 'id_Q1',\n",
        "             'RelatedPostId': 'id_Q2',\n",
        "             'LinkType': 'entailment',\n",
        "             'Id_Post': 'answer_id_Q1',\n",
        "             'Text_Post': 'answer_body_Q1',\n",
        "             'Id_Related': 'answer_id_Q2',\n",
        "             'Text_Related': 'answer_body_Q2'})\n",
        "Pairs_with_AcceptedAnswers.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z6jvX3btz4Jz"
      },
      "outputs": [],
      "source": [
        "columns = ['Id', 'AcceptedAnswerId', 'CreationDate', 'Body', 'Tags', 'Title', 'AccountId']      # 'Community'\n",
        "Questions = pd.read_csv('/content/drive/MyDrive/SE-PQA/questions_with_answer.csv', usecols=columns)\n",
        "display(Questions.head(3))\n",
        "print(\"Total number of questions with accepted answer = \", len(Questions))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JF2pPxWD5Eu4"
      },
      "outputs": [],
      "source": [
        "MyData = Pairs_with_AcceptedAnswers.merge(\n",
        "    Questions, left_on='id_Q1', right_on='Id', how='inner', suffixes=('', '_Q1')\n",
        ")\n",
        "\n",
        "MyData = MyData[['id_Q1',\t'id_Q2', 'entailment',\n",
        "                 'answer_id_Q1', 'answer_body_Q1',\n",
        "                 'answer_id_Q2', 'answer_body_Q2',\n",
        "                 'CreationDate', 'Body', 'Title',\n",
        "                 'Tags',\t'AcceptedAnswerId', 'AccountId']]\n",
        "MyData.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JSRXu8gv8Li9"
      },
      "outputs": [],
      "source": [
        "MyData = MyData.merge(\n",
        "    Questions, left_on='id_Q2', right_on='Id', how='inner', suffixes=('_Q1', '_Q2')\n",
        ")\n",
        "\n",
        "MyData = MyData[['id_Q1',\t'id_Q2', 'entailment',\n",
        "                 'answer_id_Q1', 'answer_body_Q1',\n",
        "                 'answer_id_Q2', 'answer_body_Q2',\n",
        "                 'CreationDate_Q1', 'Body_Q1', 'Title_Q1',\n",
        "                 'Tags_Q1',\t'AcceptedAnswerId_Q1', 'AccountId_Q1',\n",
        "                 'CreationDate_Q2', 'Body_Q2', 'Title_Q2',\n",
        "                 'Tags_Q2',\t'AcceptedAnswerId_Q2', 'AccountId_Q2']]\n",
        "MyData.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pxSXJEsj8uig"
      },
      "outputs": [],
      "source": [
        "MyData = MyData.rename(\n",
        "    columns={'CreationDate_Q1': 'creationDate_Q1',\n",
        "             'CreationDate_Q2': 'creationDate_Q2',\n",
        "             'Body_Q1': 'body_Q1',\n",
        "             'Body_Q2': 'body_Q2',\n",
        "             'Title_Q1': 'title_Q1',\n",
        "             'Title_Q2': 'title_Q2',\n",
        "             'Tags_Q1': 'tags_Q1',\n",
        "             'Tags_Q2': 'tags_Q2',\n",
        "             'AccountId_Q1': 'userid_Q1',\n",
        "             'AccountId_Q2': 'userid_Q2',\n",
        "             'AcceptedAnswerId_Q1': 'acceptedAnswerId_Q1',\n",
        "             'AcceptedAnswerId_Q2': 'acceptedAnswerId_Q2'\n",
        "             })\n",
        "MyData.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n_1lHVR4_Meg"
      },
      "outputs": [],
      "source": [
        "MyData = MyData[\n",
        "    (MyData[\"acceptedAnswerId_Q1\"] == MyData[\"answer_id_Q1\"]) &\n",
        "    (MyData[\"acceptedAnswerId_Q2\"] == MyData[\"answer_id_Q2\"])\n",
        "]\n",
        "MyData.head(4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OUwWwQI89eRj"
      },
      "outputs": [],
      "source": [
        "MyData.to_pickle(\"/content/drive/MyDrive/SE-PQA/SE_PQA_Data_All.pkl\")\n",
        "print(\"The number of rows in the dataset = \", len(MyData))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AQ5UvBm8ABRu"
      },
      "outputs": [],
      "source": [
        "def remove_html_tags(text):\n",
        "    return BeautifulSoup(text, \"html.parser\").get_text()\n",
        "\n",
        "def clean_text(text):\n",
        "    text = BeautifulSoup(text, \"html.parser\").get_text()\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "    return text.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hQRO178TA3d0"
      },
      "outputs": [],
      "source": [
        "MyData.loc[:, 'body_Q1'] = MyData['body_Q1'].apply(remove_html_tags)\n",
        "MyData.loc[:, 'body_Q2'] = MyData['body_Q2'].apply(remove_html_tags)\n",
        "MyData.loc[:, 'body_Q1'] = MyData['body_Q1'].apply(clean_text)\n",
        "MyData.loc[:, 'body_Q2'] = MyData['body_Q2'].apply(clean_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iXt1gT92Cd5F"
      },
      "outputs": [],
      "source": [
        "MyData.to_pickle(\"/content/drive/MyDrive/SE-PQA/SE_PQA_Data_All_cleaned.pkl\")\n",
        "display(MyData.head(3))\n",
        "print(\"The number of rows in the dataset = \", len(MyData))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GNaaVF5qHeia"
      },
      "outputs": [],
      "source": [
        "MyData = pd.read_pickle(\"/content/drive/MyDrive/SE-PQA/SE_PQA_Data_All_cleaned.pkl\")\n",
        "display(MyData.head(3))\n",
        "print(\"The number of rows in the dataset = \", len(MyData))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yJI3tzZOGHe0"
      },
      "outputs": [],
      "source": [
        "MyData2 = MyData[MyData['body_Q1'].str.split().apply(len) < 500]\n",
        "MyData2 = MyData2[MyData2['body_Q2'].str.split().apply(len) < 500]\n",
        "display(MyData2.head(3))\n",
        "print(\"The number of rows in the dataset = \", len(MyData2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L9Xnno3kICgb"
      },
      "outputs": [],
      "source": [
        "MyData2.to_pickle(\"/content/drive/MyDrive/SE-PQA/SE_PQA_Data_All_cleaned_Len500.pkl\")\n",
        "display(MyData2.head(3))\n",
        "print(\"The number of rows in the dataset = \", len(MyData2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vsq3clSsMRt8"
      },
      "outputs": [],
      "source": [
        "MyData2 = pd.read_pickle(\"/content/drive/MyDrive/SE-PQA/SE_PQA_Data_All_cleaned_Len500.pkl\")\n",
        "display(MyData2.head(3))\n",
        "print(\"The number of rows in the dataset = \", len(MyData2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ilm2GZxJIJpc"
      },
      "outputs": [],
      "source": [
        "MyData2['entailment'] = MyData2['entailment'].replace('duplicated', 'positive')\n",
        "MyData2.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bYCGb9ndJXRT"
      },
      "outputs": [],
      "source": [
        "positive_samples = MyData2.sample(n=5000, random_state=42)\n",
        "positive_samples.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Drkf6bZaJBEt"
      },
      "outputs": [],
      "source": [
        "q1_ids = positive_samples['id_Q1'].unique()\n",
        "q2_ids = positive_samples['id_Q2'].unique()\n",
        "\n",
        "all_pairs = pd.MultiIndex.from_product([q1_ids, q2_ids], names=['id_Q1', 'id_Q2']).to_frame(index=False)\n",
        "all_pairs = all_pairs[all_pairs['id_Q1'] != all_pairs['id_Q2']]\n",
        "existing_pairs = MyData[['id_Q1', 'id_Q2']]\n",
        "\n",
        "all_pairs.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JlbHPOFuPzIc"
      },
      "outputs": [],
      "source": [
        "existing_pairs = existing_pairs[existing_pairs['id_Q1'].isin(all_pairs['id_Q1'])]\n",
        "existing_pairs = existing_pairs[existing_pairs['id_Q2'].isin(all_pairs['id_Q2'])]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Waqo0O9OMoq"
      },
      "outputs": [],
      "source": [
        "all_pairs = all_pairs.sample(n=10000, random_state=42)\n",
        "merged = all_pairs.merge(existing_pairs, how='left', indicator=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hwJj1asfREEr"
      },
      "outputs": [],
      "source": [
        "merged.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KD4TMYcjRbrk"
      },
      "outputs": [],
      "source": [
        "len(merged[merged['_merge'] == 'left_only'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zy73bZVdNQzA"
      },
      "outputs": [],
      "source": [
        "negative_samples = merged[merged['_merge'] == 'left_only'].drop(columns=['_merge'])\n",
        "negative_samples = negative_samples.sample(n=5000, random_state=42)\n",
        "\n",
        "cols_Q1 = ['id_Q1', 'answer_id_Q1', 'answer_body_Q1', 'creationDate_Q1',\n",
        "           'body_Q1', 'title_Q1', 'tags_Q1', 'acceptedAnswerId_Q1', 'userid_Q1']\n",
        "q1_info = MyData[cols_Q1].drop_duplicates(subset='id_Q1')\n",
        "negative_samples = pd.merge(negative_samples, q1_info, on='id_Q1', how='left')\n",
        "\n",
        "\n",
        "cols_Q2 = ['id_Q2', 'answer_id_Q2', 'answer_body_Q2', 'creationDate_Q2',\n",
        "           'body_Q2', 'title_Q2', 'tags_Q2', 'acceptedAnswerId_Q2', 'userid_Q2']\n",
        "q2_info = MyData[cols_Q2].drop_duplicates(subset='id_Q2')\n",
        "negative_samples = pd.merge(negative_samples, q2_info, on='id_Q2', how='left')\n",
        "\n",
        "negative_samples['entailment'] = 'negative'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p0Hn9HwSR-AH"
      },
      "outputs": [],
      "source": [
        "display(negative_samples.head(3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_2jclmVDSHec"
      },
      "outputs": [],
      "source": [
        "negative_samples = negative_samples[positive_samples.columns]\n",
        "MyData3 = pd.concat([negative_samples, positive_samples], ignore_index=True)\n",
        "display(MyData3.head(3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "54-yeLhATTcV"
      },
      "outputs": [],
      "source": [
        "MyData3 = MyData3.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "MyData3.to_pickle(\"/content/drive/MyDrive/SE-PQA/SE_PQA_Data_10000_cleaned_Len500.pkl\")\n",
        "display(MyData3.head(10))\n",
        "print(\"The number of samples in the final data = \", len(MyData3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uEbIrplLk47k"
      },
      "outputs": [],
      "source": [
        "MyData3 = pd.read_pickle(\"/content/drive/MyDrive/SE-PQA/SE_PQA_Data_10000_cleaned_Len500.pkl\")\n",
        "display(MyData3.head(10))\n",
        "print(\"The number of samples in the final data = \", len(MyData3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUW0yBzNlINa"
      },
      "source": [
        "# Tag Clustering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NVuaw_lp0ZEe"
      },
      "outputs": [],
      "source": [
        "!pip install -q networkx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hj8wjsysrKi7"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from collections import Counter\n",
        "from collections import OrderedDict\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ylGDi15dmLN9"
      },
      "outputs": [],
      "source": [
        "columns = ['Tags']\n",
        "Tags = pd.read_csv('/content/drive/MyDrive/SE-PQA/questions_with_answer.csv', usecols=columns)\n",
        "display(Tags.head(3))\n",
        "print(\"Total number of questions with accepted answer = \", len(Tags))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jkVc9SzbrOXd"
      },
      "outputs": [],
      "source": [
        "tags_lists = Tags['Tags'].str.findall(r\"<(.*?)>\")\n",
        "all_posts = tags_lists.tolist()\n",
        "print(\"Number of posts : \", len(all_posts))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6zhxxYWEtsAf"
      },
      "outputs": [],
      "source": [
        "all_posts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pdjah1aE9ces"
      },
      "outputs": [],
      "source": [
        "unique_tags = list(OrderedDict.fromkeys(tag for post in all_posts for tag in post))\n",
        "print(\"Number of unique tags: \", len(unique_tags))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1H7uxI07ub8m"
      },
      "outputs": [],
      "source": [
        "Tags.to_pickle(\"/content/drive/MyDrive/SE-PQA/tags_QwAs.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4t-7iFQr8mna"
      },
      "outputs": [],
      "source": [
        "Tags = pd.read_pickle(\"/content/drive/MyDrive/SE-PQA/tags_QwAs.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RAMWcYSo9fvY"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QunNEKvuuTLd"
      },
      "outputs": [],
      "source": [
        "co_occurrence_matrix = np.zeros((len(unique_tags), len(unique_tags)))                         # Create an empty co-occurrence matrix\n",
        "\n",
        "for post in all_posts:\n",
        "    post_tags = set(post)                                                       # Extract tags from the post\n",
        "    for tag1 in post_tags:                                                      # Iterate through pairs of tags in the post\n",
        "        for tag2 in post_tags:\n",
        "            if tag1 != tag2:\n",
        "                index1 = unique_tags.index(tag1)                                       # Find the indices of tag1 and tag2 in the 'tags' list\n",
        "                index2 = unique_tags.index(tag2)\n",
        "                co_occurrence_matrix[index1, index2] += 1                       # Update the co-occurrence matrix\n",
        "                co_occurrence_matrix[index2, index1] += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ELgW_ozzxwH0"
      },
      "outputs": [],
      "source": [
        "display(co_occurrence_matrix[0:10, 0:10])\n",
        "print(co_occurrence_matrix.shape)\n",
        "\n",
        "co_occurrence_df = pd.DataFrame(co_occurrence_matrix)\n",
        "co_occurrence_matrix_file = \"/content/drive/MyDrive/SE-PQA/co_occurrence_matrix.pkl\"\n",
        "co_occurrence_df.to_pickle(co_occurrence_matrix_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_UUOfcS6xy4W"
      },
      "outputs": [],
      "source": [
        "co_occurrence_matrix_file = \"/content/drive/MyDrive/SE-PQA/co_occurrence_matrix.pkl\"\n",
        "co_occurrence_df = pd.read_pickle(co_occurrence_matrix_file)\n",
        "co_occurrence_matrix = co_occurrence_df.to_numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fhJzOhjnx2LA"
      },
      "outputs": [],
      "source": [
        "G = nx.Graph()\n",
        "for i in range(len(unique_tags)):\n",
        "    for j in range(i + 1, len(unique_tags)):\n",
        "        if co_occurrence_matrix[i][j] > 0:\n",
        "            tag1 = unique_tags[i]\n",
        "            tag2 = unique_tags[j]\n",
        "            weight = co_occurrence_matrix[i][j]\n",
        "            G.add_edge(tag1, tag2, weight=weight)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cxhthvcX92lu"
      },
      "outputs": [],
      "source": [
        "with open(\"/content/drive/MyDrive/SE-PQA/tag_graph.pkl\", \"wb\") as f:\n",
        "    pickle.dump(G, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ei4B8fsL97F9"
      },
      "outputs": [],
      "source": [
        "with open(\"/content/drive/MyDrive/SE-PQA/tag_graph.pkl\", \"rb\") as f:\n",
        "    G = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D6pK_3vJ19Br"
      },
      "outputs": [],
      "source": [
        "degrees = dict(G.degree())\n",
        "num_nodes = len(G.nodes)\n",
        "\n",
        "average_degree = sum(degrees.values()) / len(degrees)\n",
        "max_degree = max(degrees.values())\n",
        "min_degree = min(degrees.values())\n",
        "\n",
        "print(f\"Number of Nodes: {num_nodes}\")\n",
        "print(f\"Average Degree: {average_degree}\")\n",
        "print(f\"Maximum Degree: {max_degree}\")\n",
        "print(f\"Minimum Degree: {min_degree}\")\n",
        "\n",
        "plt.figure(figsize=(8.5, 4))\n",
        "\n",
        "degree_counts = Counter(degrees.values())\n",
        "degrees, counts = zip(*sorted(degree_counts.items()))\n",
        "\n",
        "plt.subplot(2, 1, 1)\n",
        "plt.bar(degrees, counts, alpha=0.8, color=\"cyan\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.gca().spines['top'].set_visible(False)\n",
        "plt.gca().spines['right'].set_visible(False)\n",
        "\n",
        "# Subplot 2: Closer Look at Degree Distribution\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.bar(degrees[:80], counts[:80], alpha=0.8, color=\"cyan\")\n",
        "plt.xlabel('Degree')\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.gca().spines['top'].set_visible(False)\n",
        "plt.gca().spines['right'].set_visible(False)\n",
        "\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(f'/content/drive/MyDrive/SE-PQA/Graph_Degree_Distribution.tiff', dpi=300)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3x6-0el26qsD"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade node2vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "msx8cgSXAMhE"
      },
      "outputs": [],
      "source": [
        "from node2vec import Node2Vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7WqQElwm2dUo"
      },
      "outputs": [],
      "source": [
        "walk_length = 10 #80\n",
        "num_walks = 30 #60 #50 #20...\n",
        "dimensions = 128\n",
        "window = 10\n",
        "min_count = 1\n",
        "num_workers = 2\n",
        "sg = 1\n",
        "epochs = 20\n",
        "alpha = 1e-3\n",
        "p = 1\n",
        "q = 0.5 #16\n",
        "seed = 42"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a5tNYdCz2nTT"
      },
      "outputs": [],
      "source": [
        "node2vec_instance = Node2Vec(\n",
        "    G,\n",
        "    dimensions=dimensions,\n",
        "    walk_length=walk_length ,\n",
        "    num_walks=num_walks,\n",
        "    workers=num_workers,\n",
        "    p = p,\n",
        "    q = q,\n",
        "    seed = seed,\n",
        "    weight_key = 'weight'\n",
        "  )\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z2nT_HMQHvZQ"
      },
      "outputs": [],
      "source": [
        "model = node2vec_instance.fit(\n",
        "    window=window,\n",
        "    min_count=min_count,\n",
        "    sg=sg,\n",
        "    # compute_loss=True,\n",
        "    epochs = epochs,\n",
        "    alpha = alpha,\n",
        "    batch_words=4,\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3H_7NyIBAUZo"
      },
      "outputs": [],
      "source": [
        "tag_embeddings = {node: model.wv[node] for node in G.nodes()}\n",
        "tag_names = list(tag_embeddings.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G-eDmb1NNEMH"
      },
      "outputs": [],
      "source": [
        "model.wv.save_word2vec_format(f\"/content/drive/MyDrive/SE-PQA/n2v_tag_embeddings.txt\")\n",
        "model.save(f\"/content/drive/MyDrive/SE-PQA/Node2vec_model.txt\")\n",
        "\n",
        "\n",
        "tag_names_pckl = pd.DataFrame(tag_names)\n",
        "tag_names_file_name = f\"/content/drive/MyDrive/SE-PQA/n2v_tag_names.pkl\"\n",
        "tag_names_pckl.to_pickle(tag_names_file_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bcimFaqOAYzN"
      },
      "outputs": [],
      "source": [
        "Node2vec_embeddings = np.array(list(tag_embeddings.values()))\n",
        "\n",
        "Node2vec_embeddings_pckl = pd.DataFrame(Node2vec_embeddings)\n",
        "Node2vec_file_name = f\"/content/drive/MyDrive/SE-PQA/n2v_embeddings.pkl\"\n",
        "Node2vec_embeddings_pckl.to_pickle(Node2vec_file_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rZXy1n3-UVPm"
      },
      "outputs": [],
      "source": [
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    f1_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    precision_recall_fscore_support\n",
        "    )"
      ],
      "metadata": {
        "id": "AQd2kh1H890L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "edges = list(G.edges())\n",
        "non_edges = [(i, j) for i in list(G.nodes()) for j in list(G.nodes()) if not G.has_edge(i, j)]\n",
        "positive_samples = [(tag_embeddings[i], tag_embeddings[j], 1) for i, j in edges]\n",
        "negative_samples = [(tag_embeddings[i], tag_embeddings[j], 0) for i, j in non_edges]"
      ],
      "metadata": {
        "id": "c69zkwNd8m7R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_samples2 = positive_samples + negative_samples[:296748]\n",
        "np.random.shuffle(all_samples2)\n",
        "\n",
        "X = np.array([(np.concatenate((i, j))) for i, j, _ in all_samples2])\n",
        "y = np.array([label for _, _, label in all_samples2])\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "classifier = LogisticRegression(max_iter=1000)\n",
        "classifier.fit(X_train, y_train)\n",
        "\n",
        "predictions = classifier.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "print(f\"Accuracy for edge prediction: {accuracy:.2f}\")\n",
        "\n",
        "print(precision_recall_fscore_support(y_test, predictions))"
      ],
      "metadata": {
        "id": "PWWta52f9DtS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Node2vec_file_name = f\"/content/drive/MyDrive/SE-PQA/n2v_embeddings.pkl\"\n",
        "Node2vec_embeddings_pckl = pd.read_pickle(Node2vec_file_name)\n",
        "Node2vec_embeddings = Node2vec_embeddings_pckl.to_numpy()\n",
        "node_embeddings = Node2vec_embeddings\n",
        "node_embeddings.shape"
      ],
      "metadata": {
        "id": "9mknEtPF9NHe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone -q https://github.com/rapidsai/rapidsai-csp-utils.git                   # Fast t-SNE\n",
        "!python rapidsai-csp-utils/colab/pip-install.py\n",
        "import cudf\n",
        "import cuml\n",
        "import copy\n",
        "cuml.__version__\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from cuml.manifold import TSNE\n",
        "from scipy.spatial.distance import cdist\n",
        "from sklearn.metrics import (\n",
        "    silhouette_score,\n",
        "    davies_bouldin_score,\n",
        "    calinski_harabasz_score\n",
        "    )\n",
        "from scipy.cluster.hierarchy import (\n",
        "    dendrogram,\n",
        "    linkage,\n",
        "    fcluster\n",
        "    )"
      ],
      "metadata": {
        "id": "NclcEqK99pz8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rTBb4oVrKrXE"
      },
      "outputs": [],
      "source": [
        "def hierarchical_clustering(embeddings, n_clusters, metric, dist_threshold, linkage, full_tree):\n",
        "    agglomerative_cluster = AgglomerativeClustering(\n",
        "        n_clusters=None,\n",
        "        metric=metric,\n",
        "        distance_threshold=dist_threshold,\n",
        "        linkage=linkage,\n",
        "        compute_full_tree = full_tree\n",
        "    )\n",
        "    pr = agglomerative_cluster.fit_predict(embeddings)\n",
        "    model = agglomerative_cluster.fit(embeddings)\n",
        "    return pr, model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7F4E8c6dKm6x"
      },
      "outputs": [],
      "source": [
        "def perform_tsne(embeddings, perplexity):                                       # Perform t-SNE for dimensionality reduction\n",
        "    tsne = TSNE(\n",
        "        n_components=2,\n",
        "        random_state=42,\n",
        "        perplexity=perplexity,\n",
        "        n_iter=10000,\n",
        "        learning_rate = 300.0,\n",
        "        n_iter_without_progress = 3000,\n",
        "        early_exaggeration = 50,\n",
        "        method = 'barnes_hut',\n",
        "        # learning_rate_method = None\n",
        "        )\n",
        "    return tsne.fit_transform(embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "34T_YvYmKhh2"
      },
      "outputs": [],
      "source": [
        "def extract_cluster_statistics(embeddings_2d, cluster_labels, tags, tag_frequency_dict):\n",
        "    data = pd.DataFrame(columns=['X', 'Y', 'Cluster', 'Tag'])                   # Create an empty DataFrame to store data\n",
        "    cluster_means, cluster_variances, cluster_sizes = [], [], []                # Initialize lists to store cluster statistics\n",
        "    representative_samples, representative_samples_freq = {}, {}                # Initialize a dictionary to store the representative sample for each cluster\n",
        "    representative_samples_freq_2nd = {}\n",
        "    representative_samples_freq_3nd = {}\n",
        "\n",
        "    for cluster_label in np.unique(cluster_labels):\n",
        "        samples_tags = [tags[i] for i, lbl in enumerate(cluster_labels) if lbl == cluster_label]\n",
        "        indexes = [i for i, lbl in enumerate(cluster_labels) if lbl == cluster_label]\n",
        "        samples_2d = embeddings_2d[cluster_labels == cluster_label]\n",
        "\n",
        "        cluster_center = np.mean(samples_2d, axis=0)\n",
        "        distances = cdist(samples_2d, [cluster_center])\n",
        "        nearest_sample_idx = np.argmin(distances)\n",
        "        representative_samples[cluster_label] = samples_tags[nearest_sample_idx]\n",
        "\n",
        "        print(samples_tags)\n",
        "        representative_samples_freq[cluster_label] = max(samples_tags, key=lambda tag: tag_frequency_dict.get(tag, 0))\n",
        "        mostfreq_sample_idx = samples_tags.index(representative_samples_freq[cluster_label])\n",
        "\n",
        "        samples_tags_copy = copy.deepcopy(samples_tags)\n",
        "        samples_tags_copy.remove(representative_samples_freq[cluster_label])\n",
        "        representative_samples_freq_2nd[cluster_label] = max(samples_tags_copy, key=lambda tag: tag_frequency_dict.get(tag, 0))\n",
        "        mostfreq_sample_idx_2nd = samples_tags.index(representative_samples_freq_2nd[cluster_label])\n",
        "\n",
        "        samples_tags_copy.remove(representative_samples_freq_2nd[cluster_label])\n",
        "        representative_samples_freq_3nd[cluster_label] = max(samples_tags_copy, key=lambda tag: tag_frequency_dict.get(tag, 0))\n",
        "        mostfreq_sample_idx_3nd = samples_tags.index(representative_samples_freq_3nd[cluster_label])\n",
        "\n",
        "\n",
        "        distances_to_representative = cdist(samples_2d, [samples_2d[nearest_sample_idx]])\n",
        "        mean_distance = np.mean(distances_to_representative)\n",
        "        variance_distance = np.var(distances_to_representative)\n",
        "        cluster_size = len(samples_tags)\n",
        "        cluster_means.append(mean_distance)\n",
        "        cluster_variances.append(variance_distance)\n",
        "        cluster_sizes.append(cluster_size)\n",
        "\n",
        "        cluster_data = pd.DataFrame(                                            # Add data for the current cluster to the DataFrame\n",
        "            {\n",
        "                'X': samples_2d[:, 0],\n",
        "                'Y': samples_2d[:, 1],\n",
        "                # 'Cluster': samples_tags[nearest_sample_idx],\n",
        "                'Cluster': samples_tags[mostfreq_sample_idx],\n",
        "                'Tag': samples_tags,\n",
        "                'Indexes': indexes\n",
        "            }\n",
        "        )\n",
        "        data = pd.concat([data, cluster_data])\n",
        "        # data = data.append(cluster_data)\n",
        "\n",
        "\n",
        "    cluster_stats = pd.DataFrame({                                              # Create a new DataFrame for cluster statistics\n",
        "        'Cluster': np.unique(cluster_labels),\n",
        "        'Representative sample': [representative_samples[cluster] for cluster in np.unique(cluster_labels)],\n",
        "        'representative_samples_freq': [representative_samples_freq[cluster] for cluster in np.unique(cluster_labels)],\n",
        "        'representative_samples_freq_2nd': [representative_samples_freq_2nd[cluster] for cluster in np.unique(cluster_labels)],\n",
        "        'representative_samples_freq_3nd': [representative_samples_freq_3nd[cluster] for cluster in np.unique(cluster_labels)],\n",
        "        'Mean_Distance': cluster_means,\n",
        "        'Variance_Distance': cluster_variances,\n",
        "        'Cluster_Size': cluster_sizes,\n",
        "    }).set_index('Cluster', drop=True)\n",
        "\n",
        "    silhouette_metric = silhouette_score(\n",
        "        embeddings_2d,\n",
        "        cluster_labels\n",
        "        )\n",
        "    calinski_harabasz_metric = calinski_harabasz_score(\n",
        "        embeddings_2d,\n",
        "        cluster_labels\n",
        "        )\n",
        "    davies_bouldin_metric = davies_bouldin_score(\n",
        "        embeddings_2d,\n",
        "        cluster_labels\n",
        "        )\n",
        "    return cluster_stats, silhouette_metric, calinski_harabasz_metric, davies_bouldin_metric, data, len(np.unique(cluster_labels))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from matplotlib import cm\n",
        "from matplotlib.colors import to_hex\n",
        "\n",
        "# Generate a distinct color palette\n",
        "def generate_distinct_colors(n_clusters):\n",
        "    cmap = cm.get_cmap('tab20', n_clusters)  # 'tab20' or 'tab20c' are good options for distinct colors\n",
        "    return [to_hex(cmap(i)) for i in range(cmap.N)]"
      ],
      "metadata": {
        "id": "V98zbIs1tFqu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CR4oumC0Kfd0"
      },
      "outputs": [],
      "source": [
        "def visualize_clusters(data, representative_samples, custom_color_scale):       # Create an interactive scatter plot\n",
        "    fig = px.scatter(\n",
        "        data,\n",
        "        x='X',\n",
        "        y='Y',\n",
        "        color= 'Cluster',\n",
        "        hover_data=['Tag'],\n",
        "        labels={'X': 'Dimension 1', 'Y': 'Dimension 2'},\n",
        "        # color_continuous_scale=custom_color_scale\n",
        "        color_discrete_sequence=custom_color_scale,\n",
        "    )\n",
        "    fig.update_traces(\n",
        "        marker=dict(size=2),\n",
        "        selector=dict(mode='markers+text')\n",
        "    )\n",
        "    fig.update_layout(showlegend=False)\n",
        "\n",
        "    # Loop over representative samples and assign unique cluster IDs\n",
        "    for counter, (cluster_label, sample_tag) in enumerate(representative_samples.items()):\n",
        "        representative_sample = data[data['Tag'] == sample_tag]\n",
        "        cluster_color = custom_color_scale[counter % len(custom_color_scale)]\n",
        "\n",
        "        fig.add_trace(go.Scatter(\n",
        "            x=[representative_sample['X'].values[0]],\n",
        "            y=[representative_sample['Y'].values[0]],\n",
        "            mode=\"markers\",\n",
        "            marker=dict(\n",
        "                size=5,\n",
        "                color='white',\n",
        "                line=dict(width=2, color=cluster_color)\n",
        "            ),\n",
        "            showlegend=False,\n",
        "            hoverinfo=\"text\"\n",
        "        ))\n",
        "\n",
        "    fig.update_layout(\n",
        "        width=1000,\n",
        "        height=800,\n",
        "        plot_bgcolor='rgba(255,255,255,255)'\n",
        "    )\n",
        "    fig.update_xaxes({'gridcolor': 'lightgray', 'zerolinecolor': 'lightgray'})\n",
        "    fig.update_yaxes({'gridcolor': 'lightgray', 'zerolinecolor': 'lightgray'})\n",
        "\n",
        "    return fig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PjSolGu9B4Tv"
      },
      "outputs": [],
      "source": [
        "n_clusters=None\n",
        "metric= 'euclidean'  #  'cosine'\n",
        "linkage_= 'ward'       #'complete' 'average'\n",
        "full_tree = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B6aF-L7h2CgA"
      },
      "outputs": [],
      "source": [
        "def plot_dendrogram(model, **kwargs):\n",
        "    counts = np.zeros(model.children_.shape[0])\n",
        "    n_samples = len(model.labels_)\n",
        "    for i, merge in enumerate(model.children_):\n",
        "        current_count = 0\n",
        "        for child_idx in merge:\n",
        "            if child_idx < n_samples:\n",
        "                current_count += 1  # leaf node\n",
        "            else:\n",
        "                current_count += counts[child_idx - n_samples]\n",
        "        counts[i] = current_count\n",
        "\n",
        "    linkage_matrix = np.column_stack(\n",
        "        [model.children_, model.distances_, counts]\n",
        "    ).astype(float)\n",
        "\n",
        "    dendrogram(linkage_matrix, **kwargs)                                        # Plot the corresponding dendrogram\n",
        "\n",
        "    return linkage_matrix"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "_, model_Agg = hierarchical_clustering(                                             # Perform hierarchical clustering\n",
        "        embeddings = node_embeddings,\n",
        "        # embeddings = embeddings,\n",
        "        n_clusters = n_clusters,\n",
        "        metric = metric,\n",
        "        dist_threshold = 0,                                                     # setting distance_threshold=0 ensures we compute the full tree.\n",
        "        linkage = linkage_,\n",
        "        full_tree = full_tree\n",
        "    )\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.title(\"Hierarchical Clustering Dendrogram\")\n",
        "linkage_matrix = plot_dendrogram(model_Agg)\n",
        "plt.xticks([])\n",
        "plt.xlabel('')\n",
        "plt.savefig(f\"/content/drive/MyDrive/SE-PQA/n2v_old_Dendogram.tiff\", dpi=300)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JlKjNGAH3ir4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "node_embeddings=np.array(node_embeddings)\n",
        "node_embeddings"
      ],
      "metadata": {
        "id": "0HpeZEHDA2zD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tag_frequency = Counter(tag for post in all_posts for tag in post)                  # Step 1: Create a dictionary to store the frequency of each tag\n",
        "tag_frequency_dict = dict(tag_frequency)                                        # Step 2: Count the frequency of each tag across all posts"
      ],
      "metadata": {
        "id": "R-ypU2rGBEpz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.io as pio\n",
        "import plotly.express as px\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "bdwVm8arBa75"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.options.display.max_rows = 200\n",
        "thresholds = [60, 35, 17]                                                       # Define a list of threshold values for the three levels\n",
        "# embeddings=np.array(list(tag_embeddings.values()))\n",
        "\n",
        "all_cluster_labels, representative_samples, Whole_data = [], {}, []             # Create a list to store cluster labels at each level\n",
        "                                                                                # and a dictionary to store representative sample names\n",
        "\n",
        "embeddings_2d = perform_tsne(                                                   # Perform t-SNE\n",
        "        embeddings=node_embeddings,\n",
        "        # embeddings=embeddings,\n",
        "        perplexity=30\n",
        "    )\n",
        "\n",
        "\n",
        "for level, threshold in enumerate(thresholds):\n",
        "    print(f\"Threshold: {threshold}\")\n",
        "\n",
        "    # Cut the dendrogram into clusters at the current threshold\n",
        "    cluster_labels = fcluster(\n",
        "        linkage_matrix,\n",
        "        t=threshold,\n",
        "        criterion='distance'\n",
        "        )\n",
        "    all_cluster_labels.append(cluster_labels)\n",
        "\n",
        "    cluster_stats, silhouette_metric,\\\n",
        "     calinski_harabasz_metric, davies_bouldin_metric,\\\n",
        "      data, num_classes = extract_cluster_statistics(                                        # Extract cluster statistics\n",
        "        embeddings_2d=embeddings_2d,\n",
        "        cluster_labels=cluster_labels,\n",
        "        tags=list(G.nodes()), #<==\n",
        "        tag_frequency_dict=tag_frequency_dict\n",
        "    )\n",
        "\n",
        "    Whole_data.append(data)\n",
        "    for c_id, rep_name in cluster_stats['representative_samples_freq'].items():       # Store representative sample names with level information\n",
        "      if level==1:\n",
        "          if any(value == rep_name for (key_level, _), value in representative_samples.items() if key_level == level-1):\n",
        "            representative_samples[(level, c_id)] = cluster_stats['representative_samples_freq_2nd'][c_id]\n",
        "          else:\n",
        "            representative_samples[(level, c_id)] = rep_name\n",
        "      elif level==2:\n",
        "          if any(value == rep_name for (key_level, _), value in representative_samples.items() if key_level == level-1):\n",
        "              representative_samples[(level, c_id)] = cluster_stats['representative_samples_freq_3nd'][c_id]\n",
        "          elif any(value == rep_name for (key_level, _), value in representative_samples.items() if key_level == level-2):\n",
        "              representative_samples[(level, c_id)] = cluster_stats['representative_samples_freq_3nd'][c_id]\n",
        "          else:\n",
        "            representative_samples[(level, c_id)] = rep_name\n",
        "      else:\n",
        "          representative_samples[(level, c_id)] = rep_name\n",
        "\n",
        "    # for c_id, rep_name in cluster_stats['Representative sample'].items():       # Store representative sample names with level information\n",
        "    #   representative_samples[(level, c_id)] = rep_name\n",
        "\n",
        "    print(\"Cluster Statistics:\")\n",
        "    display(cluster_stats)\n",
        "    print(\"Silhouette Metric:\", silhouette_metric)\n",
        "    print(\"Calinski Harabasz Metric:\", calinski_harabasz_metric)\n",
        "    print(\"Davies Bouldin Metric:\", davies_bouldin_metric)\n",
        "\n",
        "\n",
        "    # Visualize clusters\n",
        "    custom_color_scale = px.colors.sample_colorscale(\"Turbo\", [n/num_classes for n in range(num_classes)])\n",
        "    fig = visualize_clusters(\n",
        "        data=data,\n",
        "        representative_samples=cluster_stats['representative_samples_freq'].to_dict(),\n",
        "        # representative_samples=cluster_stats['Representative sample'].to_dict(),\n",
        "        custom_color_scale=custom_color_scale\n",
        "    )\n",
        "    file_path = f\"/content/drive/MyDrive/SE-PQA/n2v_old_clustering_level{level}.html\"\n",
        "    pio.write_html(fig, file_path)\n",
        "    fig.show()\n",
        "\n",
        "\n",
        "    # Visualize clusters\n",
        "    custom_color_scale = px.colors.sequential.Turbo\n",
        "    fig = visualize_clusters(\n",
        "        data=data,\n",
        "        representative_samples=cluster_stats['representative_samples_freq'].to_dict(),\n",
        "        # representative_samples=cluster_stats['Representative sample'].to_dict(),\n",
        "        custom_color_scale=custom_color_scale\n",
        "    )\n",
        "    file_path = f\"/content/drive/MyDrive/SE-PQA/n2v_old_clustering_level{level}-b.html\"\n",
        "    pio.write_html(fig, file_path)\n",
        "    fig.show()\n",
        "\n",
        "    custom_color_scale = generate_distinct_colors(num_classes)\n",
        "    fig = visualize_clusters(\n",
        "        data=data,\n",
        "        representative_samples=cluster_stats['representative_samples_freq'].to_dict(),\n",
        "        # representative_samples=cluster_stats['Representative sample'].to_dict(),\n",
        "        custom_color_scale=custom_color_scale\n",
        "    )\n",
        "    file_path = f\"/content/drive/MyDrive/SE-PQA/n2v_old_clustering_level{level}-c.html\"\n",
        "    pio.write_html(fig, file_path)\n",
        "    fig.show()"
      ],
      "metadata": {
        "id": "TQqdsSuqA5W-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hierarchical_data = []\n",
        "for tag_idx, tag in enumerate(list(G.nodes())):\n",
        "    level_1_cluster = representative_samples.get((0, all_cluster_labels[0][tag_idx]))\n",
        "    level_2_cluster = representative_samples.get((1, all_cluster_labels[1][tag_idx]))\n",
        "    level_3_cluster = representative_samples.get((2, all_cluster_labels[2][tag_idx]))\n",
        "    hierarchical_data.append([level_1_cluster, level_2_cluster, level_3_cluster, tag])\n",
        "\n",
        "print(\"Hierarchical DataFrame:\")\n",
        "hierarchical_df = pd.DataFrame(hierarchical_data, columns=[\"Level 1\", \"Level 2\", \"Level 3\", \"Tag\"])\n",
        "display(hierarchical_df)"
      ],
      "metadata": {
        "id": "rbgChGc-CyzS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xF2IQQrRc-8c"
      },
      "outputs": [],
      "source": [
        "hierarchical_df_file_name = f\"/content/drive/MyDrive/SE-PQA/n2v_old_hierarchical_df.pkl\"\n",
        "hierarchical_df.to_pickle(hierarchical_df_file_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tjOVWV9T3CNX"
      },
      "outputs": [],
      "source": [
        "hierarchical_df_file_name = f\"/content/drive/MyDrive/SE-PQA/n2v_old_hierarchical_df.pkl\"\n",
        "hierarchical_df = pd.read_pickle(hierarchical_df_file_name)\n",
        "hierarchical_df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade numpy"
      ],
      "metadata": {
        "id": "gr7nCrvWFC9i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "TCWma8mEFWxX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_path_LLama = f\"/content/drive/MyDrive/SE-PQA/SE_PQA_Data_10000_cleaned_Len500.pkl\"\n",
        "MyData_LLama = pd.read_pickle(data_path_LLama)\n",
        "MyData_LLama.head(3)"
      ],
      "metadata": {
        "id": "vGL5UKegDnfg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TG_Data = MyData_LLama[['body_Q1', 'tags_Q1']]\n",
        "TG_Data = TG_Data.rename(columns={'body_Q1': 'text', 'tags_Q1': 'tags'})\n",
        "TG_Data['tags'] = TG_Data['tags'].str.findall(r\"<(.*?)>\").apply(lambda x: ', '.join(x))\n",
        "display(TG_Data)"
      ],
      "metadata": {
        "id": "UjvAbtgcEp3p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hierarchical_df_file_name = f\"/content/drive/MyDrive/SE-PQA/n2v_old_hierarchical_df.pkl\"\n",
        "hierarchical_df = pd.read_pickle(hierarchical_df_file_name)"
      ],
      "metadata": {
        "id": "YzUfE3ArGzIo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Create a mapping dictionary from hierarchical_df\n",
        "tag_mapping = {}\n",
        "for index, row in hierarchical_df.iterrows():\n",
        "    tags = [row['Level 1'], row['Level 2'], row['Level 3']]\n",
        "    tag_mapping[row['Tag']] = ', '.join(tags)\n",
        "\n",
        "# Step 2: Process tags in TG_Data\n",
        "result_tags_list = []\n",
        "oldtags_list = []\n",
        "tags_with_duplicates_list = []\n",
        "\n",
        "for tags_str in TG_Data['tags']:\n",
        "    tags = tags_str.split(', ')\n",
        "    processed_tags = [tag_mapping.get(tag, tag) if len(tag_mapping.get(tag, tag))!=0 else tag for tag in tags]\n",
        "    result_tags_list.append(', '.join(processed_tags))\n",
        "    oldtags_list.append(', '.join(tags))\n",
        "    tags_with_duplicates_list.append(', '.join(processed_tags))\n",
        "\n",
        "# Step 3: Create result_dataframe with sorted newtags, tag frequencies, oldtags, and tags_with_duplicates\n",
        "result_dataframe = pd.DataFrame({\n",
        "    'text': TG_Data['text'],\n",
        "    'oldtags': oldtags_list,\n",
        "    'newtags': result_tags_list,\n",
        "    'tags_with_duplicates': tags_with_duplicates_list\n",
        "})\n",
        "\n",
        "# Calculate tag frequencies using Counter\n",
        "tag_frequencies_list = [dict(Counter(tags.split(', '))) for tags in result_tags_list]\n",
        "result_dataframe['tag_frequencies'] = tag_frequencies_list\n",
        "\n",
        "# Sort newtags based on frequencies, with the condition to maintain the original order\n",
        "result_dataframe['newtags'] = result_dataframe.apply(lambda row: ', '.join(sorted(set(row['newtags'].split(', ')), key=lambda tag: (row['tag_frequencies'].get(tag, 0), -row['newtags'].split(', ').index(tag)), reverse=True)), axis=1)\n",
        "\n",
        "\n",
        "# Reorder the columns as per the desired output\n",
        "result_dataframe = result_dataframe[['text', 'oldtags', 'newtags', 'tags_with_duplicates', 'tag_frequencies']]\n",
        "\n",
        "# Display the result\n",
        "TG_Data_After_HieClustering_file_name = f\"/content/drive/MyDrive/SE-PQA/n2v_old_TG_Data_After_HieClustering.pkl\"\n",
        "result_dataframe.to_pickle(TG_Data_After_HieClustering_file_name)\n",
        "display(result_dataframe)\n"
      ],
      "metadata": {
        "id": "nmxCAuJwGnxt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "MyData = pd.read_pickle(f\"/content/drive/MyDrive/SE-PQA/n2v_old_TG_Data_After_HieClustering.pkl\")\n",
        "len_tr = int(0.8 * MyData.shape[0])\n",
        "len_te = int(0.2 * MyData.shape[0])\n",
        "train_data, test_data = train_test_split(MyData,\n",
        "                                          test_size=len_te,\n",
        "                                          random_state=42)\n",
        "# train_data = MyData\n",
        "# test_data = MyData"
      ],
      "metadata": {
        "id": "jTL_ZFX4MtB8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    \"/content/drive/MyDrive/llama-2-7b-chat-hf\",\n",
        "    padding_side='left'\n",
        ")\n",
        "tokenizer.pad_token_id = 0"
      ],
      "metadata": {
        "id": "Zg4u40-rNP3a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors as mcol\n",
        "import numpy as np\n",
        "\n",
        "color_train = (255 / 255, 0 / 255, 0 / 255)\n",
        "color_test = (0 / 255, 176 / 255, 240 / 255)\n",
        "\n",
        "def count_tokens_in_tags(tags):\n",
        "    tag_list = tags.split(', ')\n",
        "    return tag_list\n",
        "\n",
        "def y_axis_formatter(x, pos):\n",
        "    return f\"{int(x / 100)}\"\n",
        "\n",
        "def prepare_data(data, tokenizer):\n",
        "    data['token_oldtags'] = data['oldtags'].apply(lambda x: len(tokenizer.tokenize(x)))\n",
        "    oldtags_tokens = data['token_oldtags'].value_counts().sort_index()\n",
        "\n",
        "    data['num_oldtags'] = data['oldtags'].apply(lambda x: len(x.split(', ')))\n",
        "    oldtags_values = data['num_oldtags'].value_counts().sort_index()\n",
        "    return oldtags_values, data['token_oldtags']\n",
        "\n",
        "\n",
        "def plot_combined(train_data, test_data, tokenizer):\n",
        "    train_oldtags, train_oldtags_tokens = prepare_data(train_data, tokenizer)\n",
        "    test_oldtags, test_oldtags_tokens = prepare_data(test_data, tokenizer)\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(10, 3), sharey=False)\n",
        "    bar_width = 0.5\n",
        "    overlap_offset = bar_width * 0.25\n",
        "\n",
        "    x_train = train_oldtags.index - overlap_offset\n",
        "    x_test = train_oldtags.index + overlap_offset\n",
        "\n",
        "    bars_train = axes[0].bar(\n",
        "        x_train,\n",
        "        train_oldtags.values,\n",
        "        width=bar_width,\n",
        "        fill=False,\n",
        "        hatch=\"///\",\n",
        "        edgecolor=color_train,\n",
        "        alpha=0.8,\n",
        "        label=\"Train\"\n",
        "    )\n",
        "    bars_test = axes[0].bar(\n",
        "        x_test,\n",
        "        test_oldtags.loc[train_oldtags.index].fillna(0).values,\n",
        "        width=bar_width,\n",
        "        hatch=\"..\",\n",
        "        color='white',\n",
        "        edgecolor=color_test,\n",
        "        label=\"Test\"\n",
        "    )\n",
        "    axes[0].set_xlabel('Number of tags')\n",
        "    axes[0].set_ylabel('Number of samples')\n",
        "\n",
        "    axes[0].annotate(\n",
        "        r\"$\\times 10^2$\",\n",
        "        xy=(0, 1),\n",
        "        xycoords='axes fraction',\n",
        "        xytext=(-10, 5),\n",
        "        textcoords='offset points',\n",
        "        ha='left',\n",
        "        va='center',\n",
        "        fontsize=9\n",
        "    )\n",
        "\n",
        "    c = [color_train, 'black']\n",
        "    h = [170, 170, 40, 40]\n",
        "    i = 0\n",
        "    for bars, data, color, offset in [(bars_train, train_oldtags, color_train, 0), (bars_test, test_oldtags, color_test, 0.15)]:\n",
        "        for bar, value in zip(bars, data.values):\n",
        "            percentage = value / sum(data.values)\n",
        "            axes[0].text(\n",
        "                bar.get_x() + bar.get_width() / 2 + offset,\n",
        "                bar.get_height() + h[i+2],\n",
        "                f\"{percentage:.1%}\",\n",
        "                ha='center',\n",
        "                va='bottom',\n",
        "                fontsize=9,\n",
        "                color=color,\n",
        "                rotation = 90\n",
        "            )\n",
        "        i += 1\n",
        "    token_data = [train_oldtags_tokens.values, test_oldtags_tokens.values]\n",
        "    box = axes[1].boxplot(\n",
        "        token_data,\n",
        "        vert=False,\n",
        "        patch_artist=True,\n",
        "        notch=True,\n",
        "        medianprops=dict(color=\"black\", linewidth=1.5),\n",
        "        whiskerprops=dict(color=color_train),\n",
        "        capprops=dict(color=color_train)\n",
        "    )\n",
        "    hatches = ['///', '..']\n",
        "    colors = [color_train, color_test]\n",
        "\n",
        "    for patch, hatch, color in zip(box['boxes'], hatches, colors):\n",
        "        patch.set_facecolor('white')\n",
        "        patch.set_edgecolor(color)\n",
        "        patch.set_hatch(hatch)\n",
        "\n",
        "\n",
        "    for i, color in enumerate(colors):\n",
        "        box['whiskers'][2 * i].set_color(color)\n",
        "        box['whiskers'][2 * i + 1].set_color(color)\n",
        "        box['caps'][2 * i].set_color(color)\n",
        "        box['caps'][2 * i + 1].set_color(color)\n",
        "\n",
        "    means = [np.mean(train_oldtags_tokens.values), np.mean(test_oldtags_tokens.values)]\n",
        "    for i, (mean, label, color) in enumerate(zip(means, [\"Train\", \"Test\"], colors)):\n",
        "        y_position = i + 1\n",
        "        axes[1].axvline(\n",
        "            mean,\n",
        "            color=color,\n",
        "            linestyle=\"--\",\n",
        "            linewidth=1,\n",
        "            ymin=0,\n",
        "            ymax=(y_position + 0) / len(token_data)\n",
        "        )\n",
        "\n",
        "        axes[1].text(\n",
        "            mean + 0.3,\n",
        "            y_position + 0.2,\n",
        "            f\"Mean: {mean:.1f}\",\n",
        "            va=\"center\",\n",
        "            ha=\"left\",\n",
        "            fontsize=9,\n",
        "            color=color\n",
        "        )\n",
        "\n",
        "    axes[1].set_xlabel('Number of tokens')\n",
        "\n",
        "    fig.legend(\n",
        "        loc='upper center',\n",
        "        bbox_to_anchor=(0.5, 1.05),\n",
        "        ncol=2,\n",
        "        fontsize=10,\n",
        "        edgecolor='black'\n",
        "    )\n",
        "\n",
        "    axes[0].yaxis.set_major_formatter(y_axis_formatter)\n",
        "    for ax in axes:\n",
        "        ax.spines['top'].set_visible(False)\n",
        "        ax.spines['right'].set_visible(False)\n",
        "\n",
        "    for i, ax in enumerate(axes):\n",
        "        x_min, x_max = ax.get_xlim()\n",
        "        y_min, y_max = ax.get_ylim()\n",
        "        ax.annotate('', xy=(x_min, y_max), xytext=(x_min, y_max - (y_max - y_min) * 0.02),\n",
        "                    arrowprops=dict(facecolor='black', arrowstyle='-|>'))\n",
        "        ax.annotate('', xy=(x_max, y_min), xytext=(x_max - (x_max - x_min) * 0.02, y_min),\n",
        "                    arrowprops=dict(facecolor='black', arrowstyle='-|>'))\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.98])\n",
        "    plt.savefig(\n",
        "        \"/content/drive/MyDrive/SE-PQA/Data_statistics(Combined).tiff\",\n",
        "        format='tiff', dpi=300, bbox_inches='tight'\n",
        "    )\n",
        "    plt.show()\n",
        "\n",
        "plot_combined(train_data, test_data, tokenizer)\n"
      ],
      "metadata": {
        "id": "euL2Ub5lJVn0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors as mcol\n",
        "import numpy as np\n",
        "\n",
        "color_train = (255 / 255, 0 / 255, 0 / 255)\n",
        "color_test = (0 / 255, 176 / 255, 240 / 255)\n",
        "\n",
        "def count_tokens_in_tags(tags):\n",
        "    tag_list = tags.split(', ')\n",
        "    return tag_list\n",
        "\n",
        "\n",
        "def y_axis_formatter(x, pos):\n",
        "    return f\"{int(x / 100)}\"\n",
        "\n",
        "def prepare_data(data, tokenizer):\n",
        "\n",
        "    data['token_newtags'] = data['newtags'].apply(lambda x: len(tokenizer.tokenize(x)))\n",
        "    newtags_tokens = data['token_newtags'].value_counts().sort_index()\n",
        "\n",
        "    data['num_newtags'] = data['newtags'].apply(lambda x: len(x.split(', ')))\n",
        "    newtags_values = data['num_newtags'].value_counts().sort_index()\n",
        "    return newtags_values, data['token_newtags']\n",
        "\n",
        "\n",
        "def plot_combined(train_data, test_data, tokenizer):\n",
        "    train_newtags, train_newtags_tokens = prepare_data(train_data, tokenizer)\n",
        "    test_newtags, test_newtags_tokens = prepare_data(test_data, tokenizer)\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(10, 3), sharey=False)\n",
        "    bar_width = 0.5\n",
        "    overlap_offset = bar_width * 0.25\n",
        "\n",
        "    aligned_test_newtags = test_newtags.reindex(train_newtags.index, fill_value=0)\n",
        "    x_train = train_newtags.index - overlap_offset\n",
        "    x_test = train_newtags.index + overlap_offset\n",
        "\n",
        "    bars_train = axes[0].bar(\n",
        "        x_train,\n",
        "        train_newtags.values,\n",
        "        width=bar_width,\n",
        "        fill=False,\n",
        "        hatch=\"///\",\n",
        "        edgecolor=color_train,\n",
        "        alpha=0.8,\n",
        "        label=\"Train\"\n",
        "    )\n",
        "    bars_test = axes[0].bar(\n",
        "        x_test,\n",
        "        aligned_test_newtags.values,\n",
        "        width=bar_width,\n",
        "        hatch=\"..\",\n",
        "        color='white',\n",
        "        edgecolor=color_test,\n",
        "        label=\"Test\"\n",
        "    )\n",
        "    axes[0].set_xlabel('Number of tags')\n",
        "    axes[0].set_ylabel('Number of samples')\n",
        "    axes[0].set_xticks(np.arange(1, 13))\n",
        "    axes[0].set_xticklabels(np.arange(1, 13), ha='right')\n",
        "\n",
        "    axes[0].annotate(\n",
        "        r\"$\\times 10^2$\",\n",
        "        xy=(0, 1),\n",
        "        xycoords='axes fraction',\n",
        "        xytext=(-10, 5),\n",
        "        textcoords='offset points',\n",
        "        ha='left',\n",
        "        va='center',\n",
        "        fontsize=9\n",
        "    )\n",
        "\n",
        "    c = [color_train, 'black']\n",
        "    h = [150, 150, 60, 600]\n",
        "    i = 0\n",
        "    for bars, data, color, offset in [(bars_train, train_newtags, color_train, 0), (bars_test, test_newtags, color_test, 0.3)]:\n",
        "        for bar, value in zip(bars, data.values):\n",
        "            percentage = value / sum(data.values)\n",
        "            axes[0].text(\n",
        "                bar.get_x() + bar.get_width() / 2 + offset,\n",
        "                bar.get_height() + h[i],\n",
        "                f\"{percentage:.1%}\",\n",
        "                ha='center',\n",
        "                va='bottom',\n",
        "                fontsize=9,\n",
        "                color=color,\n",
        "                rotation = 90\n",
        "            )\n",
        "        i += 1\n",
        "\n",
        "    token_data = [train_newtags_tokens.values, test_newtags_tokens.values]\n",
        "    box = axes[1].boxplot(\n",
        "        token_data,\n",
        "        vert=False,\n",
        "        patch_artist=True,\n",
        "        notch=True,\n",
        "        medianprops=dict(color=\"black\", linewidth=1.5),\n",
        "        whiskerprops=dict(color=color_train),\n",
        "        capprops=dict(color=color_train)\n",
        "    )\n",
        "    hatches = ['///', '..']\n",
        "    colors = [color_train, color_test]\n",
        "\n",
        "    for patch, hatch, color in zip(box['boxes'], hatches, colors):\n",
        "        patch.set_facecolor('white')\n",
        "        patch.set_edgecolor(color)\n",
        "        patch.set_hatch(hatch)\n",
        "\n",
        "    for i, color in enumerate(colors):\n",
        "        box['whiskers'][2 * i].set_color(color)\n",
        "        box['whiskers'][2 * i + 1].set_color(color)\n",
        "        box['caps'][2 * i].set_color(color)\n",
        "        box['caps'][2 * i + 1].set_color(color)\n",
        "\n",
        "    means = [np.mean(train_newtags_tokens.values), np.mean(test_newtags_tokens.values)]\n",
        "    for i, (mean, label, color) in enumerate(zip(means, [\"Train\", \"Test\"], colors)):\n",
        "        y_position = i + 1\n",
        "        axes[1].axvline(\n",
        "            mean,\n",
        "            color=color,\n",
        "            linestyle=\"--\",\n",
        "            linewidth=1,\n",
        "            ymin=0,\n",
        "            ymax=(y_position + 0) / len(token_data)\n",
        "        )\n",
        "        axes[1].text(\n",
        "            mean + 0.2,\n",
        "            y_position + 0.2,\n",
        "            f\"Mean: {mean:.1f}\",\n",
        "            va=\"center\",\n",
        "            ha=\"left\",\n",
        "            fontsize=9,\n",
        "            color=color\n",
        "        )\n",
        "\n",
        "\n",
        "    axes[1].set_xlabel('Number of tokens')\n",
        "\n",
        "    fig.legend(\n",
        "        loc='upper center',\n",
        "        bbox_to_anchor=(0.5, 1.05),\n",
        "        ncol=2,\n",
        "        fontsize=10,\n",
        "        edgecolor='black'\n",
        "    )\n",
        "\n",
        "    axes[0].yaxis.set_major_formatter(y_axis_formatter)\n",
        "\n",
        "    for ax in axes:\n",
        "        ax.spines['top'].set_visible(False)\n",
        "        ax.spines['right'].set_visible(False)\n",
        "\n",
        "    for i, ax in enumerate(axes):\n",
        "        x_min, x_max = ax.get_xlim()\n",
        "        y_min, y_max = ax.get_ylim()\n",
        "        ax.annotate('', xy=(x_min, y_max), xytext=(x_min, y_max - (y_max - y_min) * 0.02),\n",
        "                    arrowprops=dict(facecolor='black', arrowstyle='-|>'))\n",
        "        ax.annotate('', xy=(x_max, y_min), xytext=(x_max - (x_max - x_min) * 0.02, y_min),\n",
        "                    arrowprops=dict(facecolor='black', arrowstyle='-|>'))\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.98])\n",
        "    plt.savefig(\n",
        "        \"/content/drive/MyDrive/SE-PQA/Data_statistics(Combined)2.tiff\",\n",
        "        format='tiff', dpi=300, bbox_inches='tight'\n",
        "    )\n",
        "    plt.show()\n",
        "plot_combined(train_data, test_data, tokenizer)\n"
      ],
      "metadata": {
        "id": "IJ6F2PZqMgoM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train LLaMA-2 (TG)\n"
      ],
      "metadata": {
        "id": "c-KgoXA-HMjz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cXoy3AYTBIr3"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U transformers         # ==4.31.0\n",
        "!pip install -q torchmetrics\n",
        "!pip install -q pytorch_lightning\n",
        "!pip install -q bitsandbytes\n",
        "!pip install -q -U peft                    # ==0.4.0\n",
        "!pip install -q accelerate              # ==0.21.0\n",
        "!pip install -q trl\n",
        "!pip install -q tensorboard\n",
        "!pip install -q datasets\n",
        "!pip install -q rouge\n",
        "!pip install -q bert-score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iiX1X7U3USQu"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import gc\n",
        "import re\n",
        "import torch\n",
        "import warnings\n",
        "import nltk\n",
        "import json\n",
        "import time\n",
        "import requests\n",
        "nltk.download('punkt')\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import bitsandbytes as bnb\n",
        "import pytorch_lightning as pl\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CyZM5lzmASd0"
      },
      "outputs": [],
      "source": [
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "from pytorch_lightning import Callback\n",
        "from tensorboard import notebook\n",
        "\n",
        "from torchmetrics import MetricCollection\n",
        "from torchmetrics.text.bert import BERTScore\n",
        "from torchmetrics.text.rouge import ROUGEScore\n",
        "from torchmetrics.classification import (\n",
        "    BinaryAccuracy,\n",
        "    BinaryPrecision,\n",
        "    BinaryRecall,\n",
        "    BinaryF1Score\n",
        "    )\n",
        "\n",
        "from peft import (\n",
        "    TaskType,\n",
        "    PeftModel,\n",
        "    PeftConfig,\n",
        "    LoraConfig,\n",
        "    get_peft_model,\n",
        "    AutoPeftModelForCausalLM,\n",
        "    prepare_model_for_kbit_training,\n",
        "    )\n",
        "\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoConfig,\n",
        "    AutoModelForCausalLM,\n",
        "    HfArgumentParser,\n",
        "    TrainingArguments,\n",
        "    )\n",
        "\n",
        "from dataclasses import dataclass, field\n",
        "from nltk.tokenize import word_tokenize\n",
        "from typing import Optional\n",
        "from tqdm import tqdm\n",
        "from bert_score import BERTScorer\n",
        "from rouge import Rouge\n",
        "from statistics import mean\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "tqdm.pandas()\n",
        "warnings.filterwarnings('ignore')\n",
        "import transformers\n",
        "print(transformers.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CbyTvzmMvGiF"
      },
      "outputs": [],
      "source": [
        "B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
        "B_INST, E_INST = \"[INST]\", \"[/INST]\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "betBMcTQvTkW"
      },
      "outputs": [],
      "source": [
        "def get_tg_prompt(_question, _tags = None):\n",
        "  system_prompt = 'You are a Tag Generator. Respond only with a list of tags; do not include any additional text or explanations.'\n",
        "  user_prompt = f'''Please generate at least 5 tags for the provided question. Tags can include multi-word phrases if appropriate and should help hierarchically categorize the question's topics.\n",
        "### Question:\n",
        "{_question}\n",
        "### Tags:\n",
        "'''\n",
        "  prompt = f\"{B_INST} {B_SYS}{system_prompt}{E_SYS}{user_prompt} {E_INST}\\n\\n\"\n",
        "  if _tags: prompt += f'{_tags}</s>'\n",
        "  return prompt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(MyModel.tokenizer('''You are a Tag Generator. Respond only with a list of tags; do not include any additional text or explanations.\n",
        "  Please generate at least 5 tags for the provided question. Tags can include multi-word phrases if appropriate and should help hierarchically categorize the question's topics.\n",
        "### Question:\n",
        "### Tags:\n",
        "''')['input_ids'])"
      ],
      "metadata": {
        "id": "LfguXxN1sNgz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e5Ae4n-vgUdR"
      },
      "outputs": [],
      "source": [
        "def get_response_index(_input_ids, _task):\n",
        "  _index = None\n",
        "  _skip_tokens = None\n",
        "  if _task == 'RQE':\n",
        "    _index = 2\n",
        "    _skip_tokens = 10\n",
        "  if _task == 'SUM':\n",
        "    _index = 1\n",
        "    _skip_tokens = 11\n",
        "  if _task == 'TG':\n",
        "    _index = 1 #1\n",
        "    _skip_tokens = 10 #10\n",
        "  hashtags_indexes = [i for i, n in enumerate(_input_ids) if n == 29937]\n",
        "  if len(hashtags_indexes) > _index:\n",
        "    return [i for i, n in enumerate(_input_ids) if n == 29937][_index] + _skip_tokens\n",
        "  elif _task == 'RQE':\n",
        "    return 0\n",
        "  else:\n",
        "    return -1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0cIoAmSFvXVq"
      },
      "outputs": [],
      "source": [
        "def generate_prompt(data, tokenizer, is_eval):\n",
        "  promp = None\n",
        "  q1 = tokenizer.decode(tokenizer(data['text'])['input_ids'][:380],\n",
        "                        skip_special_tokens=True,\n",
        "                        clean_up_tokenization_spaces=True)\n",
        "  if is_eval: prompt = get_tg_prompt(q1)\n",
        "  else: prompt = get_tg_prompt(q1, data['tags'])\n",
        "  return prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WeMlef8LBt4X"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class ScriptArguments:\n",
        "    # ##########################################################################\n",
        "    #                             Configuration\n",
        "    # ##########################################################################\n",
        "    model_name: Optional[str] = field(\n",
        "        default = f\"/content/drive/MyDrive/llama-2-7b-chat-hf\",\n",
        "        metadata = {\"help\": \"The model that you want to train from the Hugging Face hub.\"}\n",
        "      )\n",
        "    adapter_name: Optional[str] = field(\n",
        "        default = \"LLama-TG\",\n",
        "        metadata = {\"help\": \"The adapter name saved in the HuggingFace hub.\"}\n",
        "      )\n",
        "    save_to: Optional[str] = field(\n",
        "        default = \"Drive\",                                                      # Save to \"Hub\", or \"Drive\", or \"Both\"\n",
        "        metadata = {\"help\": \"Determine where to save Adapters\"}\n",
        "      )\n",
        "    # ##########################################################################\n",
        "    #                         Logs and Checkpoints\n",
        "    # ##########################################################################\n",
        "    logging_steps: Optional[int] = field(\n",
        "        default = 1,\n",
        "        metadata = {\"help\": \"log every X update steps\"}\n",
        "      )\n",
        "    output_dir: Optional[str] = field(\n",
        "        default = \"/content/SE-PQA\",\n",
        "        metadata = {\"help\": \"the output directory for both logs and checkpoints\"}\n",
        "      )\n",
        "    every_n_epochs : Optional[int] = field(\n",
        "        default = 1,\n",
        "        metadata = {\"help\": \"Save checkpoints every X epochs\"}\n",
        "      )\n",
        "    save_on_train_epoch_end: Optional[bool] = field(\n",
        "        default = None,\n",
        "        metadata = {\"help\": \"Whether to run checkpointing at the end of training epochs or validation\"}\n",
        "      )\n",
        "    total_num_samples: Optional[str] = field(\n",
        "        default = 'All',                                                        # Use {your desired number of samples} or 'All'\n",
        "        metadata = {\"help\": \"Number of samples to be selected from the whole dataset\"}\n",
        "      )\n",
        "    # ##########################################################################\n",
        "    #                             Hyper-parameters\n",
        "    # ##########################################################################\n",
        "    max_epochs: Optional[int] = field(\n",
        "        default = 10,\n",
        "        metadata = {\"help\": \"maximum number of training epochs.\"}\n",
        "      )\n",
        "    learning_rate: Optional[float] = field(\n",
        "        default = 1e-4,\n",
        "        metadata = {\"help\": \"the learning rate\"}\n",
        "      )\n",
        "    gradient_accumulation_steps: Optional[int] = field(\n",
        "        default = 8,\n",
        "        metadata = {\"help\": \"the number of gradient accumulation steps\"}\n",
        "      )\n",
        "    gradient_checkpointing: Optional[bool] = field(\n",
        "        default = True,\n",
        "        metadata = {\"help\": \"Enables gradient checkpointing.\"}\n",
        "      )\n",
        "    per_device_train_batch_size: Optional[int] = field(\n",
        "        default = 4,\n",
        "        metadata = {\"help\": \"batch_size of training (per device)\"}\n",
        "      )\n",
        "    per_device_eval_batch_size: Optional[int] = field(\n",
        "        default = 1,\n",
        "        metadata = {\"help\": \"batch_size of validation (per device)\"}\n",
        "      )\n",
        "    max_seq_length: Optional[int] = field(\n",
        "        default = 512,\n",
        "        metadata = {\"help\": \"maximum input sequence length\"}\n",
        "      )\n",
        "    trust_remote_code: Optional[bool] = field(\n",
        "        default = True,\n",
        "        metadata = {\"help\": '''Enable `trust_remote_code` so that it\n",
        "        will execute code present on the Hub on your local machine'''}\n",
        "      )\n",
        "    split_ratio: Optional[float] = field(\n",
        "        default = (0.8, 0.2, 0),\n",
        "        metadata = {\"help\": \"train/test/validation splits\"}\n",
        "      )\n",
        "    precision: Optional[int] = field(\n",
        "        default = 16,\n",
        "        metadata = {\"help\": \"train with 16/32/bf16 precision.\"}\n",
        "      )\n",
        "    num_sanity_val_steps: Optional[float] = field(\n",
        "        default = 0,\n",
        "        metadata = {\"help\": \"number of validation batches before the first training epoch\"}\n",
        "      )\n",
        "    max_new_tokens: Optional[int] = field(\n",
        "        default = 30,\n",
        "        metadata = {\"help\": \"the maximum number of new tokens in the generated sequences (test step)\"}\n",
        "      )\n",
        "    # ##########################################################################\n",
        "    #                             Lora Configuration\n",
        "    # ##########################################################################\n",
        "    use_peft: Optional[bool] = field(\n",
        "        default = True,\n",
        "        metadata = {\"help\": \"Wether to use PEFT or not to train adapters\"}\n",
        "      )\n",
        "    lora_r: Optional[int] = field(\n",
        "        default = 64,\n",
        "        metadata = {\"help\": \"the r parameter of the LoRA adapters\"}\n",
        "      )\n",
        "    lora_alpha: Optional[int] = field(\n",
        "        default = 64,\n",
        "        metadata = {\"help\": \"the alpha parameter of the LoRA adapters\"}\n",
        "      )\n",
        "    lora_dropout: Optional[int] = field(\n",
        "        default = 0.1,\n",
        "        metadata = {\"help\": \"the dropout rate of the LoRA adapters\"}\n",
        "      )\n",
        "    # ##########################################################################\n",
        "    #                                 BitsAndBytes\n",
        "    # ##########################################################################\n",
        "    load_in_8bit: Optional[bool] = field(\n",
        "        default = False,\n",
        "        metadata = {\"help\": \"load the model in 8 bits precision\"}\n",
        "      )\n",
        "    load_in_4bit: Optional[bool] = field(\n",
        "        default = False,\n",
        "        metadata = {\"help\": \"load the model in 4 bits precision\"}\n",
        "      )\n",
        "    use_nested_quant: Optional[bool] = field(\n",
        "        default = False,\n",
        "        metadata = {\"help\": \"Activate nested quantization for 4bit base models\"}\n",
        "      )\n",
        "    bnb_4bit_compute_dtype: Optional[str] = field(\n",
        "        default = \"float16\",\n",
        "        metadata = {\"help\": \"Compute dtype for 4bit base models\"}\n",
        "      )\n",
        "    bnb_4bit_quant_type: Optional[str] = field(\n",
        "        default = \"nf4\",\n",
        "        metadata = {\"help\": \"Quantization type fp4 or nf4\"}\n",
        "      )\n",
        "\n",
        "parser = HfArgumentParser(ScriptArguments)\n",
        "script_args = parser.parse_args_into_dataclasses(return_remaining_strings=True)[0]\n",
        "pl.seed_everything(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MMU0d3K93G5i"
      },
      "outputs": [],
      "source": [
        "class OverrideEpochStepCallback(Callback):\n",
        "    def __init__(self) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "    def on_train_epoch_end(self, trainer: pl.Trainer, pl_module: pl.LightningModule):\n",
        "        self._log_step_as_current_epoch(trainer, pl_module)\n",
        "\n",
        "    def on_test_epoch_end(self, trainer: pl.Trainer, pl_module: pl.LightningModule):\n",
        "        self._log_step_as_current_epoch(trainer, pl_module)\n",
        "\n",
        "    def on_validation_epoch_end(self, trainer: pl.Trainer, pl_module: pl.LightningModule):\n",
        "        self._log_step_as_current_epoch(trainer, pl_module)\n",
        "\n",
        "    def _log_step_as_current_epoch(self, trainer: pl.Trainer, pl_module: pl.LightningModule):\n",
        "        pl_module.log(\"step\", trainer.current_epoch + 1)\n",
        "\n",
        "checkpoint_callback = ModelCheckpoint(every_n_epochs=script_args.every_n_epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EiM-793khoXQ"
      },
      "outputs": [],
      "source": [
        "class TGModel(pl.LightningModule):\n",
        "    def __init__(self, script_args):\n",
        "        super(TGModel, self).__init__()\n",
        "        self.save_hyperparameters()\n",
        "        self.Setup(script_args)\n",
        "        self.rouge = ROUGEScore()\n",
        "        self.adapter_name = script_args.adapter_name\n",
        "        self.epoch_n = 1\n",
        "\n",
        "    def Setup(self, script_args):\n",
        "        if script_args.load_in_4bit and script_args.load_in_8bit:\n",
        "          raise ValueError(\"You can't load the model in 8 bits and 4 bits at the same time\")\n",
        "        elif script_args.load_in_4bit:\n",
        "          compute_dtype = getattr(torch, script_args.bnb_4bit_compute_dtype)\n",
        "\n",
        "          bnb_config = BitsAndBytesConfig(\n",
        "              load_in_4bit = script_args.load_in_4bit,\n",
        "              bnb_4bit_quant_type = script_args.bnb_4bit_quant_type,\n",
        "              bnb_4bit_compute_dtype = compute_dtype,\n",
        "              bnb_4bit_use_double_quant = script_args.use_nested_quant,\n",
        "          )\n",
        "          self.model = AutoModelForCausalLM.from_pretrained(\n",
        "              script_args.model_name,\n",
        "              quantization_config = bnb_config,\n",
        "              device_map = {\"\": 0},\n",
        "          )\n",
        "        elif script_args.load_in_8bit:\n",
        "          self.model = AutoModelForCausalLM.from_pretrained(\n",
        "              script_args.model_name,\n",
        "              load_in_8bit = True,\n",
        "              torch_dtype = torch.float16,\n",
        "              device_map = {\"\": 0},\n",
        "          )\n",
        "          self.model = prepare_model_for_kbit_training(self.model)\n",
        "        else:\n",
        "          self.model = AutoModelForCausalLM.from_pretrained(\n",
        "              script_args.model_name,\n",
        "              torch_dtype = torch.bfloat16,\n",
        "              device_map = {\"\": 0},\n",
        "          )\n",
        "\n",
        "        if script_args.use_peft:\n",
        "            lora_config = LoraConfig(\n",
        "                task_type = TaskType.CAUSAL_LM,\n",
        "                r = script_args.lora_r,\n",
        "                lora_alpha = script_args.lora_alpha,\n",
        "                lora_dropout = script_args.lora_dropout,\n",
        "                bias = \"none\",\n",
        "            )\n",
        "            self.model = get_peft_model(self.model, lora_config)\n",
        "            self.model.print_trainable_parameters()\n",
        "\n",
        "        self.model.config.use_cache = False\n",
        "\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
        "            script_args.model_name,\n",
        "            padding_side='left'\n",
        "        )\n",
        "        self.tokenizer.pad_token_id = 0\n",
        "        self.model.config.pad_token_id = self.tokenizer.pad_token_id\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels=None):\n",
        "        output = self.model(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            labels=labels\n",
        "            )\n",
        "        return output.loss, output.logits\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        input_ids = batch['input_ids']\n",
        "        attention_mask = batch['attention_mask']\n",
        "        labels = batch['labels']\n",
        "\n",
        "        loss, _ = self.forward(input_ids, attention_mask, labels)\n",
        "        self.log('train_loss', loss.item(), on_epoch=True, on_step=True)\n",
        "        return loss\n",
        "\n",
        "\n",
        "    def on_train_epoch_end(self):\n",
        "      out_dir = f\"/content/drive/MyDrive/SE-PQA/TG-Adapters/\"\n",
        "      self.model.save_pretrained(out_dir + self.adapter_name + str(self.epoch_n))\n",
        "      self.epoch_n += 1\n",
        "\n",
        "    def generate(self, *args, **kwargs):\n",
        "      return self.model.generate(*args, **kwargs)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.AdamW(self.model.parameters(), lr=script_args.learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MyModel = TGModel(script_args)\n",
        "logger = TensorBoardLogger(script_args.output_dir + 'logs', name=\"TG\")\n",
        "\n",
        "print(MyModel)\n",
        "print(\"#\"*60, \"\\n\\t\\t\\t Model Configuration\\n\", \"#\"*60)\n",
        "print(MyModel.model.config)"
      ],
      "metadata": {
        "id": "tLRi4uE_IgsM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MyData = pd.read_pickle(f\"/content/drive/MyDrive/SE-PQA/n2v_old_TG_Data_After_HieClustering.pkl\")\n",
        "MyData = MyData[['text', 'newtags']]\n",
        "MyData.rename(columns = {'newtags': 'tags'}, inplace = True)\n",
        "\n",
        "MyData[\"text\"] = MyData[\"text\"].str.replace(r\"^\\s*Possible Duplicates?:\\s+.*?\\s{2,}.*?\\s{2,}\", \"\", regex=True)\n",
        "MyData[\"text\"] = MyData[\"text\"].str.replace(r\"^\\s*Possible Duplicates?:\\s+.*?\\s{2,}\", \"\", regex=True)\n",
        "MyData[\"text\"] = MyData[\"text\"].str.replace(\"C#\", \"C\", regex=False)\n",
        "MyData[\"text\"] = MyData[\"text\"].str.replace(r\"#\", \"\", regex=True)\n",
        "MyData[\"text\"] = MyData[\"text\"].str.replace(r\"\\n\", \" \", regex=True)\n",
        "\n",
        "\n",
        "if script_args.total_num_samples != 'All':\n",
        "  MyData = MyData[:int(script_args.total_num_samples)]\n",
        "\n",
        "print(MyData.shape)\n",
        "display(MyData[0:100])"
      ],
      "metadata": {
        "id": "ymAztrQIKBmH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7zvhk7mmiFfo"
      },
      "outputs": [],
      "source": [
        "class TGDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data, tokenizer, max_len, is_eval):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "        self.is_eval = is_eval\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "      row_data = self.data.iloc[index]\n",
        "      prompt = generate_prompt(row_data, self.tokenizer, self.is_eval)\n",
        "      prompt_encoding = self.tokenizer(\n",
        "          prompt,\n",
        "          max_length = self.max_len,\n",
        "          padding = 'max_length',\n",
        "          truncation = True,\n",
        "          add_special_tokens = True,\n",
        "          return_tensors = 'pt',\n",
        "      )\n",
        "      input_ids = prompt_encoding['input_ids'].squeeze()\n",
        "      attention_mask = prompt_encoding['attention_mask'].squeeze()\n",
        "\n",
        "      if self.is_eval == False:\n",
        "        response_index = get_response_index(input_ids, 'TG')\n",
        "        if response_index:\n",
        "          labels = torch.cat((torch.full((response_index,), -100), input_ids[response_index:])).squeeze()\n",
        "        else:\n",
        "          print('response_index not found')\n",
        "      else:\n",
        "        labels = self.tokenizer(\n",
        "            row_data['tags'] + '</s>',\n",
        "            add_special_tokens = False,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "        labels = labels['input_ids'].squeeze()\n",
        "      return {\n",
        "          'input_ids': input_ids,\n",
        "          'attention_mask': attention_mask,\n",
        "          'labels': labels\n",
        "      }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6h-Li53GiLvZ"
      },
      "outputs": [],
      "source": [
        "class TGDataModule(pl.LightningDataModule):\n",
        "    def __init__(self, data, tokenizer, script_args):\n",
        "        super().__init__()\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.per_device_train_batch_size = script_args.per_device_train_batch_size\n",
        "        self.per_device_eval_batch_size = script_args.per_device_eval_batch_size\n",
        "        self.max_len = script_args.max_seq_length\n",
        "        self.setup()\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        len_tr = int(script_args.split_ratio[0] * self.data.shape[0])\n",
        "        len_te = int(script_args.split_ratio[1] * self.data.shape[0])\n",
        "        train_data, test_data = train_test_split(self.data,\n",
        "                                                 test_size=len_te,\n",
        "                                                 random_state=42)\n",
        "        train_data.reset_index(drop=True, inplace=True)\n",
        "        test_data.reset_index(drop=True, inplace=True)\n",
        "\n",
        "        self.train_data = TGDataset(train_data, self.tokenizer, self.max_len, is_eval=False)\n",
        "        self.test_data = TGDataset(test_data, self.tokenizer, self.max_len, is_eval=True)\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return torch.utils.data.DataLoader(\n",
        "            self.train_data,\n",
        "            batch_size=self.per_device_train_batch_size,\n",
        "            shuffle=True,\n",
        "            num_workers=8,\n",
        "        )\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return torch.utils.data.DataLoader(\n",
        "            self.test_data,\n",
        "            sampler = torch.utils.data.SequentialSampler(self.test_data,),\n",
        "            batch_size= self.per_device_eval_batch_size,\n",
        "            num_workers=8\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bEouim7oiWKm"
      },
      "outputs": [],
      "source": [
        "MyModel.tokenizer.truncation_side = 'left'\n",
        "DataModule = TGDataModule(\n",
        "    MyData,\n",
        "    MyModel.tokenizer,\n",
        "    script_args\n",
        ")\n",
        "print(\"num train batches\", len(DataModule.train_dataloader()))\n",
        "print(\"num test batches\", len(DataModule.test_dataloader()))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for sample in DataModule.train_dataloader():\n",
        "  print(MyModel.tokenizer.batch_decode(sample['input_ids'])[0])\n",
        "  print(MyModel.tokenizer.batch_decode(sample['attention_mask'])[0])\n",
        "  print(sample['labels'][0].tolist())\n",
        "  break"
      ],
      "metadata": {
        "id": "y86krO-Aj53J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for sample in DataModule.test_dataloader():\n",
        "  print(sample)\n",
        "  print(MyModel.tokenizer.batch_decode(sample['labels']))\n",
        "  break"
      ],
      "metadata": {
        "id": "Bh0APfntcqBE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vi_Ws16M241J"
      },
      "outputs": [],
      "source": [
        "trainer = pl.Trainer(\n",
        "    logger = logger,\n",
        "    log_every_n_steps = script_args.logging_steps,\n",
        "    max_epochs = script_args.max_epochs,\n",
        "    accumulate_grad_batches = script_args.gradient_accumulation_steps,\n",
        "    num_sanity_val_steps = script_args.num_sanity_val_steps,\n",
        "    callbacks = [OverrideEpochStepCallback(), checkpoint_callback],                                  #\n",
        "    default_root_dir= script_args.output_dir + 'Checkpoints',\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e0FOy8lOVsDj"
      },
      "outputs": [],
      "source": [
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir /content/ReQuESTlogs\n",
        "\n",
        "trainer.fit(\n",
        "    MyModel,\n",
        "    datamodule=DataModule,\n",
        "    # ckpt_path = \"/content/ReQuESTlogs/TG/version_0/checkpoints/epoch=0-step=1.ckpt\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r /content/SE-PQAlogs /content/drive/MyDrive/SE-PQA/SE-PQAlogs_TG"
      ],
      "metadata": {
        "id": "bJ-8jJozRmyB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir /content/SE-PQAlogs"
      ],
      "metadata": {
        "id": "z_yemWmoS8yG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MyModel.model.save_pretrained(f\"/content/drive/MyDrive/SE-PQA/LLama-TG10\")"
      ],
      "metadata": {
        "id": "Q3KhKtYjIhQI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mhkmyo_xJESu"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "# del tokenizer\n",
        "# del trainer\n",
        "# del MyModel\n",
        "# del fModel\n",
        "# del BaseModel\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xy3--dnj1-HJ"
      },
      "outputs": [],
      "source": [
        "BaseModel= AutoModelForCausalLM.from_pretrained(\n",
        "    f\"/content/drive/MyDrive/llama-2-7b-chat-hf\",\n",
        "    device_map={\"\": 0},\n",
        "    offload_folder=\"offload\",\n",
        "    offload_state_dict = True,\n",
        "    # load_in_8bit = True,\n",
        "    )\n",
        "\n",
        "address = f\"/content/drive/MyDrive/SE-PQA/TG-Adapters/LLama-TG10\"\n",
        "print(\"\\n Loading model from \", address, \"\\n\")\n",
        "config = PeftConfig.from_pretrained(address)\n",
        "fModel= PeftModel.from_pretrained(BaseModel,address,device_map={\"\": 0})\n",
        "fModel = fModel.merge_and_unload()\n",
        "print(fModel)\n",
        "print(fModel.config)\n",
        "print(\"\\n Model successfully loded from \", address, \"\\n\")\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    script_args.model_name,\n",
        "    # add_special_tokens = False,\n",
        "    padding_side='left'\n",
        "    )\n",
        "\n",
        "tokenizer.pad_token_id = 0\n",
        "fModel.config.pad_token_id = tokenizer.pad_token_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lT1Gsgz1wMxO"
      },
      "outputs": [],
      "source": [
        "DataModule = TGDataModule(\n",
        "    MyData,\n",
        "    tokenizer,\n",
        "    script_args\n",
        ")\n",
        "print(\"num train batches\", len(DataModule.train_dataloader()))\n",
        "print(\"num test batches\", len(DataModule.test_dataloader()))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for sample in DataModule.test_dataloader():\n",
        "  # print(sample['input_ids'][0].tolist())\n",
        "  print(tokenizer.batch_decode(sample['input_ids'])[0])\n",
        "  print(tokenizer.batch_decode(sample['attention_mask'])[0])\n",
        "  print(sample['labels'][0].tolist())\n",
        "  break"
      ],
      "metadata": {
        "id": "RdF9jrhhJNEu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_step(test_dl):\n",
        "    results = []\n",
        "\n",
        "    for batch in test_dl:\n",
        "        input_ids = batch['input_ids'].cuda()\n",
        "        attention_mask = batch['attention_mask'].cuda()\n",
        "        labels = batch['labels'].cuda()\n",
        "\n",
        "        generated_txts_ids = fModel.generate(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            max_new_tokens=script_args.max_new_tokens,\n",
        "            do_sample=True,\n",
        "            temperature=0.97,\n",
        "        ).squeeze()\n",
        "\n",
        "        response_start_idx = get_response_index(\n",
        "                generated_txts_ids, 'TG'\n",
        "                )\n",
        "        single_generated_txt = tokenizer.decode(\n",
        "            generated_txts_ids[response_start_idx:],\n",
        "            skip_special_tokens=True,\n",
        "            clean_up_tokenization_spaces=True\n",
        "        )\n",
        "        # print(labels)\n",
        "        # single_label_ids = labels\n",
        "        # single_label_ids = torch.where(\n",
        "        #     single_label_ids != -100,\n",
        "        #     single_label_ids,\n",
        "        #     tokenizer.pad_token_id\n",
        "        # )\n",
        "        # single_target_txt = tokenizer.decode(\n",
        "        #     single_label_ids,\n",
        "        #     skip_special_tokens=True,\n",
        "        #     clean_up_tokenization_spaces=True\n",
        "        # )\n",
        "        # results.append([single_generated_txt, single_target_txt])\n",
        "        results.append([single_generated_txt])\n",
        "        # display(results)\n",
        "        # input()\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "CM8WsHE8wkpH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "su5vnMd1Nzai"
      },
      "outputs": [],
      "source": [
        "fModel.eval()\n",
        "testOutputs = test_step(DataModule.test_dataloader())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "testOutputs[:3]"
      ],
      "metadata": {
        "id": "Jbnq6-fveyGc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qZdUtlkIZ341"
      },
      "outputs": [],
      "source": [
        "testOutputs_file_name = f\"/content/drive/MyDrive/SE-PQA/test_outputs10.pkl\"\n",
        "testOutputs = pd.read_pickle(testOutputs_file_name)\n",
        "testOutputs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len_tr = int(script_args.split_ratio[0] * MyData.shape[0])\n",
        "len_te = int(script_args.split_ratio[1] * MyData.shape[0])\n",
        "train_data, test_data = train_test_split(MyData,\n",
        "                                          test_size=len_te,\n",
        "                                          random_state=42)\n",
        "test_data['tags'].head(3)"
      ],
      "metadata": {
        "id": "LNaKrGT_ZJe3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nY-ivGbFZSra"
      },
      "outputs": [],
      "source": [
        "testOutputs_file_name = f\"/content/drive/MyDrive/SE-PQA/test_outputs_TG.pkl\"\n",
        "testOutputs2['generated_tags'] = testOutputs\n",
        "testOutputs2['target_tags'] = test_data['tags'].reset_index(drop=True)\n",
        "testOutputs2.to_pickle(testOutputs_file_name)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "testOutputs2"
      ],
      "metadata": {
        "id": "ib3EHMhWfU1W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AuY0CkbRPqgf"
      },
      "outputs": [],
      "source": [
        "scorer = BERTScorer(lang=\"en\", device=\"cuda\")\n",
        "P, R, F1 = scorer.score(testOutputs2['generated_tags'].to_list(), testOutputs2['target_tags'].to_list(), verbose=False)\n",
        "print(f\"BERTScore Precision: {P}\")\n",
        "print(f\"BERTScore Recall: {R}\")\n",
        "print(f\"BERTScore F1: {F1}\")\n",
        "print(f\"BERTScore Precision: {P.mean():.4f}, Recall: {R.mean():.4f}, F1: {F1.mean():.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dH208JRwPwx3"
      },
      "outputs": [],
      "source": [
        "rouge = Rouge()\n",
        "# scores = rouge.get_scores(testOutputs['generated_tags'].to_list(), testOutputs['target_tags'].to_list())\n",
        "# print(scores)\n",
        "scores2 = rouge.get_scores(testOutputs2['generated_tags'].to_list(), testOutputs2['target_tags'].to_list(), avg=True)\n",
        "print(\"rouge-1:\", scores2['rouge-1'])\n",
        "print(\"rouge-2:\",scores2['rouge-2'])\n",
        "print(\"rouge-l:\",scores2['rouge-l'])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Indexer -> Retriever"
      ],
      "metadata": {
        "id": "3fyhC919NuVJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "import re"
      ],
      "metadata": {
        "id": "_RsuZsP9YVPN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "columns = ['Id', 'AcceptedAnswerId', 'CreationDate', 'Body']      # 'Community'\n",
        "Questions = pd.read_csv('/content/drive/MyDrive/SE-PQA/questions_with_answer.csv', usecols=columns)\n",
        "display(Questions.head(3))\n",
        "print(\"Total number of questions with accepted answer = \", len(Questions))"
      ],
      "metadata": {
        "id": "kw0K_PaqXG7i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Questions = Questions[Questions['Body'].str.split().apply(len) < 500]"
      ],
      "metadata": {
        "id": "ctldYluSX-BK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "columns = ['ParentId', 'Id', 'Text']\n",
        "Answers = pd.read_csv('/content/drive/MyDrive/SE-PQA/answers.csv', usecols=columns)\n",
        "Answers.head(3)"
      ],
      "metadata": {
        "id": "RQC_6t7FXCAz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Total number of answers = \", len(Answers))"
      ],
      "metadata": {
        "id": "_QIBha7Omtw7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Answers = Answers.rename(\n",
        "    columns={'Id': 'Id_Answer'})"
      ],
      "metadata": {
        "id": "2ib0RAj6mNR2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_html_tags(text):\n",
        "    return BeautifulSoup(text, \"html.parser\").get_text()\n",
        "\n",
        "def clean_text(text):\n",
        "    text = BeautifulSoup(text, \"html.parser\").get_text()\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "    return text.strip()"
      ],
      "metadata": {
        "id": "cKs09X83XwVj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Questions.loc[:, 'Body'] = Questions['Body'].apply(remove_html_tags)\n",
        "Questions.loc[:, 'Body'] = Questions['Body'].apply(clean_text)"
      ],
      "metadata": {
        "id": "vLPFRmiAXw_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Questions_with_AcceptedAnswers = Questions.merge(\n",
        "    Answers,\n",
        "    left_on=['Id', 'AcceptedAnswerId'],\n",
        "    right_on=['ParentId', 'Id_Answer'],\n",
        "    suffixes=('', '_Answer')\n",
        ")\n",
        "Questions_with_AcceptedAnswers = Questions_with_AcceptedAnswers[['Id', 'Id_Answer', 'AcceptedAnswerId', 'Text', 'CreationDate', 'Body']]\n",
        "Questions_with_AcceptedAnswers = Questions_with_AcceptedAnswers.drop_duplicates(subset=['Id', 'Id_Answer'], keep='last')\n",
        "print(len(Questions_with_AcceptedAnswers))\n",
        "display(Questions_with_AcceptedAnswers.head(3))"
      ],
      "metadata": {
        "id": "4tRQcIBJXPQP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q sentence-transformers"
      ],
      "metadata": {
        "id": "hfF5_goeqXiq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q torch"
      ],
      "metadata": {
        "id": "MTbvVesXruEz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "import time\n",
        "import torch\n",
        "from IPython.display import display, HTML"
      ],
      "metadata": {
        "id": "t0Fee2C1qbOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()\n",
        "torch.cuda.ipc_collect()"
      ],
      "metadata": {
        "id": "nrE2_jErvuwl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 512\n",
        "model = SentenceTransformer('all-mpnet-base-v2').cuda()\n",
        "\n",
        "def generate_embeddings(texts, batch_size=batch_size):\n",
        "    embeddings = model.encode(texts, convert_to_tensor=True, show_progress_bar=True, batch_size=batch_size)\n",
        "    return embeddings\n",
        "\n",
        "start_time = time.time()\n",
        "embeddings = generate_embeddings(Questions_with_AcceptedAnswers['Body'].tolist(), batch_size=batch_size)\n",
        "end_time = time.time()\n",
        "vectorize_time = end_time - start_time\n",
        "\n",
        "address2 = f\"/content/drive/MyDrive/EMNLP/Questions_with_AcceptedAnswers_mpnet.pt\"\n",
        "torch.save(embeddings, address2)\n",
        "\n",
        "Questions_with_AcceptedAnswers_vec = torch.load(address2)\n",
        "print(Questions_with_AcceptedAnswers_vec.shape)\n",
        "\n",
        "vectorize_time = vectorize_time/ len(embeddings)\n",
        "display(HTML('<span style=\"color: red\"> average vectorize time = </span><b>' + str(vectorize_time) + '</b>'))"
      ],
      "metadata": {
        "id": "OluX8CHQpxvu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "P5y-almF2Mwe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MyData3 = pd.read_pickle(\"/content/drive/MyDrive/SE-PQA/SE_PQA_Data_10000_cleaned_Len500.pkl\")\n",
        "len_tr = int(0.8 * MyData3.shape[0])\n",
        "len_te = int(0.2 * MyData3.shape[0])\n",
        "train_data, test_data = train_test_split(MyData3,\n",
        "                                          test_size=len_te,\n",
        "                                          random_state=42)\n",
        "test_data"
      ],
      "metadata": {
        "id": "Xi4ykToN2h_t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = test_data.sample(n=148, random_state=42)\n",
        "test_data.head(3)"
      ],
      "metadata": {
        "id": "awDHk74T27S6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n = 50\n",
        "cosine_sim_func = torch.nn.CosineSimilarity(dim=1, eps=1e-08)\n",
        "Questions_with_AcceptedAnswers2 = Questions_with_AcceptedAnswers.reset_index(drop=True)\n",
        "\n",
        "def get_top_n_similar_questions(row, QsAccAnsVect, QsAccAnsBody, n):\n",
        "\n",
        "    question_vector = QsAccAnsVect[QsAccAnsBody['Id']==row['id_Q1']]\n",
        "    QSA = QsAccAnsVect[(QsAccAnsBody['Id']!=row['id_Q1'])]\n",
        "    cosine_sim = cosine_sim_func(question_vector, QSA)\n",
        "    top_n_indices = np.flip(np.argsort(cosine_sim.cpu())[-n:].numpy())\n",
        "    top_n_scores = np.flip(np.sort(cosine_sim.cpu())[-n:])\n",
        "\n",
        "    similar_questions = (QsAccAnsBody[(QsAccAnsBody['Id']!=row['id_Q1'])]).reset_index(drop=True).iloc[top_n_indices]\n",
        "\n",
        "    return top_n_indices, top_n_scores, similar_questions\n",
        "\n",
        "\n",
        "\n",
        "retrival_results = []\n",
        "total_time = 0\n",
        "\n",
        "for i, row in test_data.iterrows():\n",
        "    start_time = time.time()\n",
        "    top_n_indices, cosine_sim, similar_questions = get_top_n_similar_questions(row, Questions_with_AcceptedAnswers_vec, Questions_with_AcceptedAnswers2, n)\n",
        "\n",
        "    end_time = time.time()\n",
        "    retrieval_time = end_time - start_time\n",
        "    total_time += retrieval_time\n",
        "\n",
        "    retrival_results.append({\n",
        "        'RQE Question': row['body_Q1'],\n",
        "        'RQE Answer': row['answer_body_Q1'],\n",
        "        'Cosine similarities': cosine_sim,\n",
        "        'Top n Similar Questions Body': similar_questions['Body'].tolist(),\n",
        "        'Top n Candidate Answers': similar_questions['Text'].tolist(),\n",
        "        'Top n Similar Questions (Cosine)': top_n_indices,\n",
        "        'Top n Similar Questions (id)': similar_questions['Id'].tolist(),\n",
        "        'Retrieval Time (seconds)': retrieval_time\n",
        "    })\n",
        "\n",
        "avg_retrieval_time = total_time / len(test_data)                        # Calculate average retrieval time"
      ],
      "metadata": {
        "id": "0uYSrLS02GA0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "address3 = f\"/content/drive/MyDrive/SE-PQA/retrival_results_mpnet-50.pkl\"\n",
        "(pd.DataFrame(retrival_results)).to_pickle(address3)"
      ],
      "metadata": {
        "id": "_8PEUH3p6U1a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retrival_results = pd.read_pickle(address3)\n",
        "for index, result in retrival_results.iterrows():\n",
        "    print(\"RQE Question:\", result['RQE Question'])\n",
        "    print(\"RQE Answer:\", result['RQE Answer'])\n",
        "    print('Cosine similarities:', result['Cosine similarities']),\n",
        "    print(\"Top n Similar Questions Body:\", result['Top n Similar Questions Body'])\n",
        "    print(\"Top n Similar Questions (Cosine):\", result['Top n Similar Questions (Cosine)'])\n",
        "    print(\"Top n Similar Questions (id):\", result['Top n Similar Questions (id)'])\n",
        "    print(\"Top n Candidate Answers:\", result['Top n Candidate Answers'])\n",
        "    print(\"Retrieval Time (seconds):\", result['Retrieval Time (seconds)'])\n",
        "    print(\"\\n\")\n",
        "    break"
      ],
      "metadata": {
        "id": "mcNOr5MW6mq5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#User modeling"
      ],
      "metadata": {
        "id": "g6L8i_KS6994"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "import re"
      ],
      "metadata": {
        "id": "eBuIVPVz-9wm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "columns = ['Id', 'CreationDate', 'Body', 'AccountId']      # 'Community'\n",
        "Questions = pd.read_csv('/content/drive/MyDrive/SE-PQA/questions_with_answer.csv', usecols=columns)\n",
        "display(Questions.head(3))\n",
        "print(\"Total number of questions with accepted answer = \", len(Questions))"
      ],
      "metadata": {
        "id": "GsVfOZej-9wq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Questions = Questions[Questions['Body'].str.split().apply(len) < 500]"
      ],
      "metadata": {
        "id": "3vNbz4la-9ws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_html_tags(text):\n",
        "    return BeautifulSoup(text, \"html.parser\").get_text()\n",
        "\n",
        "def clean_text(text):\n",
        "    text = BeautifulSoup(text, \"html.parser\").get_text()\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "    return text.strip()"
      ],
      "metadata": {
        "id": "jOlPi1_b-9wx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Questions.loc[:, 'Body'] = Questions['Body'].apply(remove_html_tags)\n",
        "Questions.loc[:, 'Body'] = Questions['Body'].apply(clean_text)"
      ],
      "metadata": {
        "id": "5iMAI2XW-9wx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MyData3 = pd.read_pickle(\"/content/drive/MyDrive/SE-PQA/SE_PQA_Data_10000_cleaned_Len500.pkl\")\n",
        "len(MyData3['userid_Q1'].unique())"
      ],
      "metadata": {
        "id": "NRTfiXF87KsT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_history_df = pd.DataFrame(columns=['userid', 'historyCount', 'historyIDs', 'history'])\n",
        "unique_users = pd.DataFrame(MyData3['userid_Q1'].unique(), columns=['userid_Q1'])\n",
        "\n",
        "for index, row in unique_users.iterrows():\n",
        "    user_id = row['userid_Q1']\n",
        "    user_history = (Questions[Questions['AccountId'] == user_id][['Id', 'Body']])\n",
        "    user_history = user_history.drop_duplicates(subset=['Id'])\n",
        "    user_history_str = ', '.join(user_history['Id'].astype(str))\n",
        "    user_history_body_str = ', '.join(user_history['Body'].astype(str))\n",
        "    user_history_df = pd.concat(\n",
        "        [user_history_df, pd.DataFrame({'userid': [user_id],\n",
        "                                        'historyCount': len(user_history),\n",
        "                                        'historyIDs': [user_history_str],\n",
        "                                        'history': [user_history_body_str]})\n",
        "        ], ignore_index=True)\n",
        "\n",
        "user_history_df_filepath = f\"/content/drive/MyDrive/SE-PQA/user_history_df.pkl\"\n",
        "user_history_df.to_pickle(user_history_df_filepath)\n",
        "display(user_history_df)"
      ],
      "metadata": {
        "id": "W6w1h5qBAR4f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sFb87_upCUUM"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U transformers         # ==4.31.0\n",
        "!pip install -q torchmetrics\n",
        "!pip install -q pytorch_lightning\n",
        "!pip install -q bitsandbytes\n",
        "!pip install -q -U peft                    # ==0.4.0\n",
        "!pip install -q accelerate              # ==0.21.0\n",
        "!pip install -q trl\n",
        "!pip install -q tensorboard\n",
        "!pip install -q datasets\n",
        "!pip install -q rouge\n",
        "!pip install -q bert-score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4MsXJHvyCUUN"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import gc\n",
        "import re\n",
        "import torch\n",
        "import warnings\n",
        "import nltk\n",
        "import json\n",
        "import time\n",
        "import requests\n",
        "nltk.download('punkt')\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import bitsandbytes as bnb\n",
        "import pytorch_lightning as pl\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tMsDong8CUUN"
      },
      "outputs": [],
      "source": [
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "from pytorch_lightning import Callback\n",
        "from tensorboard import notebook\n",
        "\n",
        "from torchmetrics import MetricCollection\n",
        "from torchmetrics.text.bert import BERTScore\n",
        "from torchmetrics.text.rouge import ROUGEScore\n",
        "from torchmetrics.classification import (\n",
        "    BinaryAccuracy,\n",
        "    BinaryPrecision,\n",
        "    BinaryRecall,\n",
        "    BinaryF1Score\n",
        "    )\n",
        "\n",
        "from peft import (\n",
        "    TaskType,\n",
        "    PeftModel,\n",
        "    PeftConfig,\n",
        "    LoraConfig,\n",
        "    get_peft_model,\n",
        "    AutoPeftModelForCausalLM,\n",
        "    prepare_model_for_kbit_training,\n",
        "    )\n",
        "\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoConfig,\n",
        "    AutoModelForCausalLM,\n",
        "    HfArgumentParser,\n",
        "    TrainingArguments,\n",
        "    )\n",
        "\n",
        "from dataclasses import dataclass, field\n",
        "from nltk.tokenize import word_tokenize\n",
        "from typing import Optional\n",
        "from tqdm import tqdm\n",
        "from bert_score import BERTScorer\n",
        "from rouge import Rouge\n",
        "from statistics import mean\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "tqdm.pandas()\n",
        "warnings.filterwarnings('ignore')\n",
        "import transformers\n",
        "print(transformers.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zpfZkJ5PCUUO"
      },
      "outputs": [],
      "source": [
        "B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
        "B_INST, E_INST = \"[INST]\", \"[/INST]\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rFisbuDUCUUO"
      },
      "outputs": [],
      "source": [
        "def get_tg_prompt(_question, _tags = None):\n",
        "  system_prompt = 'You are a Tag Generator. Respond only with a list of tags; do not include any additional text or explanations.'\n",
        "  user_prompt = f'''Please generate at least 5 tags for the provided question. Tags can include multi-word phrases if appropriate and should help hierarchically categorize the question's topics.\n",
        "### Question:\n",
        "{_question}\n",
        "### Tags:\n",
        "'''\n",
        "  prompt = f\"{B_INST} {B_SYS}{system_prompt}{E_SYS}{user_prompt} {E_INST}\\n\\n\"\n",
        "  if _tags: prompt += f'{_tags}</s>'\n",
        "  return prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x7hN4bJfCUUP"
      },
      "outputs": [],
      "source": [
        "def get_response_index(_input_ids, _task):\n",
        "  _index = None\n",
        "  _skip_tokens = None\n",
        "  if _task == 'RQE':\n",
        "    _index = 2\n",
        "    _skip_tokens = 10\n",
        "  if _task == 'SUM':\n",
        "    _index = 1\n",
        "    _skip_tokens = 11\n",
        "  if _task == 'TG':\n",
        "    _index = 1 #1\n",
        "    _skip_tokens = 10 #10\n",
        "  hashtags_indexes = [i for i, n in enumerate(_input_ids) if n == 29937]\n",
        "  if len(hashtags_indexes) > _index:\n",
        "    return [i for i, n in enumerate(_input_ids) if n == 29937][_index] + _skip_tokens\n",
        "  elif _task == 'RQE':\n",
        "    return 0\n",
        "  else:\n",
        "    return -1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g00GUbquCUUP"
      },
      "outputs": [],
      "source": [
        "def generate_prompt(data, tokenizer, is_eval):\n",
        "  promp = None\n",
        "  q1 = tokenizer.decode(tokenizer(data['text'])['input_ids'][:380],\n",
        "                        skip_special_tokens=True,\n",
        "                        clean_up_tokenization_spaces=True)\n",
        "  if is_eval: prompt = get_tg_prompt(q1)\n",
        "  else: prompt = get_tg_prompt(q1, data['tags'])\n",
        "  return prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rkJRJB3fCUUP"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class ScriptArguments:\n",
        "    # ##########################################################################\n",
        "    #                             Configuration\n",
        "    # ##########################################################################\n",
        "    model_name: Optional[str] = field(\n",
        "        default = f\"/content/drive/MyDrive/llama-2-7b-chat-hf\",\n",
        "        metadata = {\"help\": \"The model that you want to train from the Hugging Face hub.\"}\n",
        "      )\n",
        "    adapter_name: Optional[str] = field(\n",
        "        default = \"LLama-TG\",\n",
        "        metadata = {\"help\": \"The adapter name saved in the HuggingFace hub.\"}\n",
        "      )\n",
        "    save_to: Optional[str] = field(\n",
        "        default = \"Drive\",                                                      # Save to \"Hub\", or \"Drive\", or \"Both\"\n",
        "        metadata = {\"help\": \"Determine where to save Adapters\"}\n",
        "      )\n",
        "    # ##########################################################################\n",
        "    #                         Logs and Checkpoints\n",
        "    # ##########################################################################\n",
        "    logging_steps: Optional[int] = field(\n",
        "        default = 1,\n",
        "        metadata = {\"help\": \"log every X update steps\"}\n",
        "      )\n",
        "    output_dir: Optional[str] = field(\n",
        "        default = \"/content/SE-PQA\",\n",
        "        metadata = {\"help\": \"the output directory for both logs and checkpoints\"}\n",
        "      )\n",
        "    every_n_epochs : Optional[int] = field(\n",
        "        default = 1,\n",
        "        metadata = {\"help\": \"Save checkpoints every X epochs\"}\n",
        "      )\n",
        "    save_on_train_epoch_end: Optional[bool] = field(\n",
        "        default = None,\n",
        "        metadata = {\"help\": \"Whether to run checkpointing at the end of training epochs or validation\"}\n",
        "      )\n",
        "    total_num_samples: Optional[str] = field(\n",
        "        default = 'All',                                                        # Use {your desired number of samples} or 'All'\n",
        "        metadata = {\"help\": \"Number of samples to be selected from the whole dataset\"}\n",
        "      )\n",
        "    # ##########################################################################\n",
        "    #                             Hyper-parameters\n",
        "    # ##########################################################################\n",
        "    max_epochs: Optional[int] = field(\n",
        "        default = 10,\n",
        "        metadata = {\"help\": \"maximum number of training epochs.\"}\n",
        "      )\n",
        "    learning_rate: Optional[float] = field(\n",
        "        default = 1e-4,\n",
        "        metadata = {\"help\": \"the learning rate\"}\n",
        "      )\n",
        "    gradient_accumulation_steps: Optional[int] = field(\n",
        "        default = 8,\n",
        "        metadata = {\"help\": \"the number of gradient accumulation steps\"}\n",
        "      )\n",
        "    gradient_checkpointing: Optional[bool] = field(\n",
        "        default = True,\n",
        "        metadata = {\"help\": \"Enables gradient checkpointing.\"}\n",
        "      )\n",
        "    per_device_train_batch_size: Optional[int] = field(\n",
        "        default = 4,\n",
        "        metadata = {\"help\": \"batch_size of training (per device)\"}\n",
        "      )\n",
        "    per_device_eval_batch_size: Optional[int] = field(\n",
        "        default = 1,\n",
        "        metadata = {\"help\": \"batch_size of validation (per device)\"}\n",
        "      )\n",
        "    max_seq_length: Optional[int] = field(\n",
        "        default = 512,\n",
        "        metadata = {\"help\": \"maximum input sequence length\"}\n",
        "      )\n",
        "    trust_remote_code: Optional[bool] = field(\n",
        "        default = True,\n",
        "        metadata = {\"help\": '''Enable `trust_remote_code` so that it\n",
        "        will execute code present on the Hub on your local machine'''}\n",
        "      )\n",
        "    split_ratio: Optional[float] = field(\n",
        "        default = (0.8, 0.2, 0),\n",
        "        metadata = {\"help\": \"train/test/validation splits\"}\n",
        "      )\n",
        "    precision: Optional[int] = field(\n",
        "        default = 16,\n",
        "        metadata = {\"help\": \"train with 16/32/bf16 precision.\"}\n",
        "      )\n",
        "    num_sanity_val_steps: Optional[float] = field(\n",
        "        default = 0,\n",
        "        metadata = {\"help\": \"number of validation batches before the first training epoch\"}\n",
        "      )\n",
        "    max_new_tokens: Optional[int] = field(\n",
        "        default = 30,\n",
        "        metadata = {\"help\": \"the maximum number of new tokens in the generated sequences (test step)\"}\n",
        "      )\n",
        "    # ##########################################################################\n",
        "    #                             Lora Configuration\n",
        "    # ##########################################################################\n",
        "    use_peft: Optional[bool] = field(\n",
        "        default = True,\n",
        "        metadata = {\"help\": \"Wether to use PEFT or not to train adapters\"}\n",
        "      )\n",
        "    lora_r: Optional[int] = field(\n",
        "        default = 64,\n",
        "        metadata = {\"help\": \"the r parameter of the LoRA adapters\"}\n",
        "      )\n",
        "    lora_alpha: Optional[int] = field(\n",
        "        default = 64,\n",
        "        metadata = {\"help\": \"the alpha parameter of the LoRA adapters\"}\n",
        "      )\n",
        "    lora_dropout: Optional[int] = field(\n",
        "        default = 0.1,\n",
        "        metadata = {\"help\": \"the dropout rate of the LoRA adapters\"}\n",
        "      )\n",
        "    # ##########################################################################\n",
        "    #                                 BitsAndBytes\n",
        "    # ##########################################################################\n",
        "    load_in_8bit: Optional[bool] = field(\n",
        "        default = False,\n",
        "        metadata = {\"help\": \"load the model in 8 bits precision\"}\n",
        "      )\n",
        "    load_in_4bit: Optional[bool] = field(\n",
        "        default = False,\n",
        "        metadata = {\"help\": \"load the model in 4 bits precision\"}\n",
        "      )\n",
        "    use_nested_quant: Optional[bool] = field(\n",
        "        default = False,\n",
        "        metadata = {\"help\": \"Activate nested quantization for 4bit base models\"}\n",
        "      )\n",
        "    bnb_4bit_compute_dtype: Optional[str] = field(\n",
        "        default = \"float16\",\n",
        "        metadata = {\"help\": \"Compute dtype for 4bit base models\"}\n",
        "      )\n",
        "    bnb_4bit_quant_type: Optional[str] = field(\n",
        "        default = \"nf4\",\n",
        "        metadata = {\"help\": \"Quantization type fp4 or nf4\"}\n",
        "      )\n",
        "\n",
        "parser = HfArgumentParser(ScriptArguments)\n",
        "script_args = parser.parse_args_into_dataclasses(return_remaining_strings=True)[0]\n",
        "pl.seed_everything(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1QX6TDrICUUQ"
      },
      "outputs": [],
      "source": [
        "class OverrideEpochStepCallback(Callback):\n",
        "    def __init__(self) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "    def on_train_epoch_end(self, trainer: pl.Trainer, pl_module: pl.LightningModule):\n",
        "        self._log_step_as_current_epoch(trainer, pl_module)\n",
        "\n",
        "    def on_test_epoch_end(self, trainer: pl.Trainer, pl_module: pl.LightningModule):\n",
        "        self._log_step_as_current_epoch(trainer, pl_module)\n",
        "\n",
        "    def on_validation_epoch_end(self, trainer: pl.Trainer, pl_module: pl.LightningModule):\n",
        "        self._log_step_as_current_epoch(trainer, pl_module)\n",
        "\n",
        "    def _log_step_as_current_epoch(self, trainer: pl.Trainer, pl_module: pl.LightningModule):\n",
        "        pl_module.log(\"step\", trainer.current_epoch + 1)\n",
        "\n",
        "checkpoint_callback = ModelCheckpoint(every_n_epochs=script_args.every_n_epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SmBrjrMzCUUQ"
      },
      "outputs": [],
      "source": [
        "class TGModel(pl.LightningModule):\n",
        "    def __init__(self, script_args):\n",
        "        super(TGModel, self).__init__()\n",
        "        self.save_hyperparameters()\n",
        "        self.Setup(script_args)\n",
        "        self.rouge = ROUGEScore()\n",
        "        self.adapter_name = script_args.adapter_name\n",
        "        self.epoch_n = 1\n",
        "\n",
        "    def Setup(self, script_args):\n",
        "        if script_args.load_in_4bit and script_args.load_in_8bit:\n",
        "          raise ValueError(\"You can't load the model in 8 bits and 4 bits at the same time\")\n",
        "        elif script_args.load_in_4bit:\n",
        "          compute_dtype = getattr(torch, script_args.bnb_4bit_compute_dtype)\n",
        "\n",
        "          bnb_config = BitsAndBytesConfig(\n",
        "              load_in_4bit = script_args.load_in_4bit,\n",
        "              bnb_4bit_quant_type = script_args.bnb_4bit_quant_type,\n",
        "              bnb_4bit_compute_dtype = compute_dtype,\n",
        "              bnb_4bit_use_double_quant = script_args.use_nested_quant,\n",
        "          )\n",
        "          self.model = AutoModelForCausalLM.from_pretrained(\n",
        "              script_args.model_name,\n",
        "              quantization_config = bnb_config,\n",
        "              device_map = {\"\": 0},\n",
        "          )\n",
        "        elif script_args.load_in_8bit:\n",
        "          self.model = AutoModelForCausalLM.from_pretrained(\n",
        "              script_args.model_name,\n",
        "              load_in_8bit = True,\n",
        "              torch_dtype = torch.float16,\n",
        "              device_map = {\"\": 0},\n",
        "          )\n",
        "          self.model = prepare_model_for_kbit_training(self.model)\n",
        "        else:\n",
        "          self.model = AutoModelForCausalLM.from_pretrained(\n",
        "              script_args.model_name,\n",
        "              torch_dtype = torch.bfloat16,\n",
        "              device_map = {\"\": 0},\n",
        "          )\n",
        "\n",
        "        if script_args.use_peft:\n",
        "            lora_config = LoraConfig(\n",
        "                task_type = TaskType.CAUSAL_LM,\n",
        "                r = script_args.lora_r,\n",
        "                lora_alpha = script_args.lora_alpha,\n",
        "                lora_dropout = script_args.lora_dropout,\n",
        "                bias = \"none\",\n",
        "            )\n",
        "            self.model = get_peft_model(self.model, lora_config)\n",
        "            self.model.print_trainable_parameters()\n",
        "\n",
        "        self.model.config.use_cache = False\n",
        "\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
        "            script_args.model_name,\n",
        "            padding_side='left'\n",
        "        )\n",
        "        self.tokenizer.pad_token_id = 0\n",
        "        self.model.config.pad_token_id = self.tokenizer.pad_token_id\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels=None):\n",
        "        output = self.model(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            labels=labels\n",
        "            )\n",
        "        return output.loss, output.logits\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        input_ids = batch['input_ids']\n",
        "        attention_mask = batch['attention_mask']\n",
        "        labels = batch['labels']\n",
        "\n",
        "        loss, _ = self.forward(input_ids, attention_mask, labels)\n",
        "        self.log('train_loss', loss.item(), on_epoch=True, on_step=True)\n",
        "        return loss\n",
        "\n",
        "\n",
        "    def on_train_epoch_end(self):\n",
        "      out_dir = f\"/content/drive/MyDrive/SE-PQA/TG-Adapters/\"\n",
        "      self.model.save_pretrained(out_dir + self.adapter_name + str(self.epoch_n))\n",
        "      self.epoch_n += 1\n",
        "\n",
        "    def generate(self, *args, **kwargs):\n",
        "      return self.model.generate(*args, **kwargs)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.AdamW(self.model.parameters(), lr=script_args.learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JD2Sc63_CUUR"
      },
      "outputs": [],
      "source": [
        "class TGDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data, tokenizer, max_len, is_eval):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "        self.is_eval = is_eval\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "      row_data = self.data.iloc[index]\n",
        "      prompt = generate_prompt(row_data, self.tokenizer, self.is_eval)\n",
        "      prompt_encoding = self.tokenizer(\n",
        "          prompt,\n",
        "          max_length = self.max_len,\n",
        "          padding = 'max_length',\n",
        "          truncation = True,\n",
        "          add_special_tokens = True,\n",
        "          return_tensors = 'pt',\n",
        "      )\n",
        "      input_ids = prompt_encoding['input_ids'].squeeze()\n",
        "      attention_mask = prompt_encoding['attention_mask'].squeeze()\n",
        "\n",
        "      if self.is_eval == False:\n",
        "        response_index = get_response_index(input_ids, 'TG')\n",
        "        if response_index:\n",
        "          labels = torch.cat((torch.full((response_index,), -100), input_ids[response_index:])).squeeze()\n",
        "        else:\n",
        "          print('response_index not found')\n",
        "      else:\n",
        "        labels = self.tokenizer(\n",
        "            row_data['tags'] + '</s>',\n",
        "            add_special_tokens = False,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "        labels = labels['input_ids'].squeeze()\n",
        "      return {\n",
        "          'input_ids': input_ids,\n",
        "          'attention_mask': attention_mask,\n",
        "          'labels': labels\n",
        "      }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h-7BFBdJCUUR"
      },
      "outputs": [],
      "source": [
        "class TGDataModule(pl.LightningDataModule):\n",
        "    def __init__(self, data, tokenizer, script_args):\n",
        "        super().__init__()\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.per_device_train_batch_size = script_args.per_device_train_batch_size\n",
        "        self.per_device_eval_batch_size = script_args.per_device_eval_batch_size\n",
        "        self.max_len = script_args.max_seq_length\n",
        "        self.setup()\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        len_tr = int(script_args.split_ratio[0] * self.data.shape[0])\n",
        "        len_te = int(script_args.split_ratio[1] * self.data.shape[0])\n",
        "        train_data, test_data = train_test_split(self.data,\n",
        "                                                 test_size=len_te,\n",
        "                                                 random_state=42)\n",
        "        train_data.reset_index(drop=True, inplace=True)\n",
        "        test_data.reset_index(drop=True, inplace=True)\n",
        "\n",
        "        self.train_data = TGDataset(train_data, self.tokenizer, self.max_len, is_eval=False)\n",
        "        self.test_data = TGDataset(test_data, self.tokenizer, self.max_len, is_eval=True)\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return torch.utils.data.DataLoader(\n",
        "            self.train_data,\n",
        "            batch_size=self.per_device_train_batch_size,\n",
        "            shuffle=True,\n",
        "            num_workers=8,\n",
        "        )\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return torch.utils.data.DataLoader(\n",
        "            self.test_data,\n",
        "            sampler = torch.utils.data.SequentialSampler(self.test_data,),\n",
        "            batch_size= self.per_device_eval_batch_size,\n",
        "            num_workers=8\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AVVMiOzaCUUS"
      },
      "outputs": [],
      "source": [
        "BaseModel= AutoModelForCausalLM.from_pretrained(\n",
        "    f\"/content/drive/MyDrive/llama-2-7b-chat-hf\",\n",
        "    device_map={\"\": 0},\n",
        "    offload_folder=\"offload\",\n",
        "    offload_state_dict = True,\n",
        "    # load_in_8bit = True,\n",
        "    )\n",
        "\n",
        "address = f\"/content/drive/MyDrive/SE-PQA/TG-Adapters/LLama-TG10\"\n",
        "print(\"\\n Loading model from \", address, \"\\n\")\n",
        "config = PeftConfig.from_pretrained(address)\n",
        "fModel= PeftModel.from_pretrained(BaseModel,address,device_map={\"\": 0})\n",
        "fModel = fModel.merge_and_unload()\n",
        "print(fModel)\n",
        "print(fModel.config)\n",
        "print(\"\\n Model successfully loded from \", address, \"\\n\")\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    script_args.model_name,\n",
        "    # add_special_tokens = False,\n",
        "    padding_side='left'\n",
        "    )\n",
        "\n",
        "tokenizer.pad_token_id = 0\n",
        "fModel.config.pad_token_id = tokenizer.pad_token_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u-c54lijLeBL"
      },
      "outputs": [],
      "source": [
        "def test_step(test_dl):\n",
        "  testOutputs = []\n",
        "\n",
        "  for batch in test_dl:\n",
        "    input_ids = batch['input_ids'].cuda()\n",
        "    attention_mask = batch['attention_mask'].cuda()\n",
        "\n",
        "    generated_txts_ids = fModel.generate(\n",
        "        input_ids = input_ids,\n",
        "        max_new_tokens = script_args.max_new_tokens,\n",
        "        do_sample=True,\n",
        "        temperature=0.97\n",
        "        ).squeeze()\n",
        "\n",
        "    generated_txts = tokenizer.decode(\n",
        "        generated_txts_ids[get_response_index(generated_txts_ids, 'TG'):],\n",
        "        skip_special_tokens = False,\n",
        "        clean_up_tokenization_spaces = True\n",
        "        )\n",
        "\n",
        "    testOutputs.append(generated_txts[:-4])\n",
        "\n",
        "  return testOutputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QJLU-D82XFYb"
      },
      "outputs": [],
      "source": [
        "user_history_df_filepath = f\"/content/drive/MyDrive/SE-PQA/user_history_df.pkl\"\n",
        "user_history_df = pd.read_pickle(user_history_df_filepath)\n",
        "display(user_history_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bFxtAa1zhiWx"
      },
      "outputs": [],
      "source": [
        "start_time = time.time()\n",
        "fModel.eval()\n",
        "# user_history_df['generated_tags'] = None\n",
        "\n",
        "for index, row in user_history_df.iterrows():\n",
        "    if index>62:\n",
        "      history = row['history']\n",
        "      history_questions = pd.DataFrame(history.split(', ')[:10], columns=['text'])\n",
        "      history_questions['tags'] = \"\"\n",
        "      historytags = []\n",
        "\n",
        "      data = TGDataset(history_questions, tokenizer, 512, is_eval=True)\n",
        "      DL = torch.utils.data.DataLoader(\n",
        "              data, sampler = torch.utils.data.SequentialSampler(data),\n",
        "              batch_size= 1, num_workers=8\n",
        "          )\n",
        "      historytags = test_step(DL)\n",
        "      user_history_df.at[index, 'generated_tags']= ' -- '.join(historytags)\n",
        "\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
        "user_history_df_filepath = f\"/content/drive/MyDrive/SE-PQA/user_history_gen_tags.pkl\"\n",
        "user_history_df.to_pickle(user_history_df_filepath)\n",
        "display(user_history_df)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "user_history_df['generated_tags'] = user_history_df['generated_tags'].str.replace('/', '')"
      ],
      "metadata": {
        "id": "EMAbGqUXpcif"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w5ZvxSALHFvr"
      },
      "outputs": [],
      "source": [
        "user_history_df['generated_tags2'] = None\n",
        "user_history_df\n",
        "\n",
        "for index, row in user_history_df.iterrows():\n",
        "  A = row['generated_tags'].split(' -- ')[-10:]\n",
        "  B = ', '.join(A)\n",
        "  user_history_df.at[index, 'generated_tags2'] = B\n",
        "\n",
        "display(user_history_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YvW1waNLMI6r"
      },
      "outputs": [],
      "source": [
        "def process_row(row):\n",
        "    tags_series = pd.Series(row['generated_tags2'].split(', ')).explode()\n",
        "    tag_counts = tags_series.value_counts()\n",
        "    sorted_tags = tag_counts.index.tolist()\n",
        "    top_20_tags = sorted_tags[:20]\n",
        "    result = ', '.join(top_20_tags)\n",
        "    return result\n",
        "\n",
        "user_history_df['generated_tags2'] = user_history_df.apply(process_row, axis=1)\n",
        "user_history_df_filepath = f\"/content/drive/MyDrive/SE-PQA/user_history_T20_gen_tags.pkl\"\n",
        "user_history_df.to_pickle(user_history_df_filepath)\n",
        "display(user_history_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fhCFQ3z2TA_F"
      },
      "outputs": [],
      "source": [
        "data_path_LLama = f\"/content/drive/MyDrive/SE-PQA/SE_PQA_Data_10000_cleaned_Len500.pkl\"\n",
        "MyData_LLama = pd.read_pickle(data_path_LLama)\n",
        "MyData_LLama = MyData_LLama.merge(\n",
        "    user_history_df[['userid', 'generated_tags2']],\n",
        "    left_on='userid_Q1',\n",
        "    right_on='userid',\n",
        "    how='left'\n",
        ")\n",
        "MyData_LLama.rename(columns={'generated_tags2': 'U_Background_kn'}, inplace=True)\n",
        "MyData_LLama_filepath = f\"/content/drive/MyDrive/SE-PQA/SE_PQA_Data_10000_cleaned_Len500_T20_UK.pkl\"\n",
        "MyData_LLama.to_pickle(MyData_LLama_filepath)\n",
        "MyData_LLama"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Post-retrieval"
      ],
      "metadata": {
        "id": "QcvzDQnlB4PQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xspl8mXVCOQI"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U transformers         # ==4.31.0\n",
        "!pip install -q torchmetrics\n",
        "!pip install -q pytorch_lightning\n",
        "!pip install -q bitsandbytes\n",
        "!pip install -q -U peft                    # ==0.4.0\n",
        "!pip install -q accelerate              # ==0.21.0\n",
        "!pip install -q trl\n",
        "!pip install -q tensorboard\n",
        "!pip install -q datasets\n",
        "!pip install -q rouge\n",
        "!pip install -q bert-score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fQgB3nmiCOQK"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import torch\n",
        "import warnings\n",
        "import nltk\n",
        "import json\n",
        "import time\n",
        "import requests\n",
        "import sklearn\n",
        "import gc\n",
        "nltk.download('punkt')\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import bitsandbytes as bnb\n",
        "import pytorch_lightning as pl\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mEJj7_htCOQK"
      },
      "outputs": [],
      "source": [
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "from pytorch_lightning import Callback\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "from tensorboard import notebook\n",
        "\n",
        "from torchmetrics import MetricCollection\n",
        "from torchmetrics.text.bert import BERTScore\n",
        "from torchmetrics.text.rouge import ROUGEScore\n",
        "from torchmetrics.classification import (\n",
        "    BinaryAccuracy,\n",
        "    BinaryPrecision,\n",
        "    BinaryRecall,\n",
        "    BinaryF1Score\n",
        "    )\n",
        "\n",
        "from peft import (\n",
        "    TaskType,\n",
        "    PeftModel,\n",
        "    PeftConfig,\n",
        "    LoraConfig,\n",
        "    get_peft_model,\n",
        "    AutoPeftModelForCausalLM,\n",
        "    prepare_model_for_kbit_training,\n",
        "    )\n",
        "\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoConfig,\n",
        "    AutoModelForCausalLM,\n",
        "    HfArgumentParser,\n",
        "    TrainingArguments,\n",
        "    )\n",
        "\n",
        "from dataclasses import dataclass, field\n",
        "from nltk.tokenize import word_tokenize\n",
        "from typing import Optional\n",
        "from tqdm import tqdm\n",
        "from bert_score import BERTScorer\n",
        "from rouge import Rouge\n",
        "from statistics import mean\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorboard.backend.event_processing.event_accumulator import EventAccumulator\n",
        "\n",
        "tqdm.pandas()\n",
        "warnings.filterwarnings('ignore')\n",
        "import transformers\n",
        "print(transformers.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XxgeDmgOCOQL"
      },
      "outputs": [],
      "source": [
        "B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
        "B_INST, E_INST = \"[INST]\", \"[/INST]\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def get_rqe_prompt(_q1, _q2, _BN, _entailment=None):\n",
        "#     system_prompt = \"Help recognize question entailment\"\n",
        "#     user_prompt = f'''Entailment means:\n",
        "# 1. every answer to Q2 must be a partial or complete answer to Q1\n",
        "# 2. Q2 must be related to the topics of interest of Q1's asker, denoted by Kn.\n",
        "# Respond with \"positive\" for entailment and \"negative\" for not-entailment. No other words.\n",
        "# Example1:\n",
        "# Q1: How can I read a PDF?\n",
        "# Kn: python, programming, pandas\n",
        "# Q2: Help me how to open different files such as pdf, docx, etc in Linux?\n",
        "# Answer: negative\n",
        "\n",
        "# Example2:\n",
        "# Q1: How can I read a PDF?\n",
        "# Kn: linux, debian, filesystems\n",
        "# Q2: Help me how to open different files such as pdf, docx, etc in Linux?\n",
        "# Answer: positive\n",
        "\n",
        "# Now, evaluate the following:\n",
        "# Q1: {_q1}\n",
        "# Kn: {_BN}\n",
        "# Q2: {_q2}\n",
        "# ### Answer:\n",
        "# '''\n",
        "#     prompt = f\"{B_INST} {B_SYS}{system_prompt}{E_SYS}{user_prompt}{E_INST}\\n\\n \"\n",
        "#     if _entailment: prompt += f\"{_entailment}\"\n",
        "#     return prompt\n",
        "\n",
        "\n",
        "def get_rqe_prompt(_q1, _q2, _entailment=None):\n",
        "    system_prompt = \"Help recognize question entailment\"\n",
        "    user_prompt = f'''Entailment means every answer to Q2 must be a partial or complete answer to Q1\n",
        "Respond with \"positive\" for entailment and \"negative\" for not-entailment. No other words.\n",
        "\n",
        "Now, evaluate the following:\n",
        "Q1: {_q1}\n",
        "Q2: {_q2}\n",
        "### Answer:\n",
        "'''\n",
        "    prompt = f\"{B_INST} {B_SYS}{system_prompt}{E_SYS}{user_prompt}{E_INST}\\n\\n \"\n",
        "    if _entailment: prompt += f\"{_entailment}\"\n",
        "    return prompt"
      ],
      "metadata": {
        "id": "CgDbAalmCrRI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_response_index(_input_ids, _task):\n",
        "  _index = None\n",
        "  _skip_tokens = None\n",
        "  if _task == 'RQE':\n",
        "    _index = 0\n",
        "    _skip_tokens = 10\n",
        "  if _task == 'SUM':\n",
        "    _index = 1\n",
        "    _skip_tokens = 11\n",
        "  if _task == 'TG':\n",
        "    _index = 1\n",
        "    _skip_tokens = 10\n",
        "  hashtags_indexes = [i for i, n in enumerate(_input_ids) if n == 29937]\n",
        "  if len(hashtags_indexes) > _index:\n",
        "    return [i for i, n in enumerate(_input_ids) if n == 29937][_index] + _skip_tokens\n",
        "  elif _task == 'RQE':\n",
        "    return 0\n",
        "  else:\n",
        "    return -1"
      ],
      "metadata": {
        "id": "utCbNBmPCy7v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def generate_prompt_rqe(data, tokenizer, is_eval):\n",
        "#   promp = None\n",
        "#   q1 = tokenizer.decode(tokenizer(data['q1'])['input_ids'][:200],\n",
        "#                         skip_special_tokens=True,\n",
        "#                         clean_up_tokenization_spaces=True)\n",
        "#   q2 = tokenizer.decode(tokenizer(data['q2'])['input_ids'][:200],\n",
        "#                         skip_special_tokens=True,\n",
        "#                         clean_up_tokenization_spaces=True)\n",
        "#   ub = tokenizer.decode(tokenizer(data['U_Background_kn'])['input_ids'][:100],\n",
        "#                         skip_special_tokens=True,\n",
        "#                         clean_up_tokenization_spaces=True)\n",
        "#   if is_eval: prompt = get_rqe_prompt(q1, q2, ub)\n",
        "#   else: prompt = get_rqe_prompt(q1, q2, ub, data['entailment'])\n",
        "#   return prompt\n",
        "\n",
        "\n",
        "def generate_prompt_rqe(data, tokenizer, is_eval):\n",
        "  promp = None\n",
        "  q1 = tokenizer.decode(tokenizer(data['q1'])['input_ids'][:200],\n",
        "                        skip_special_tokens=True,\n",
        "                        clean_up_tokenization_spaces=True)\n",
        "  q2 = tokenizer.decode(tokenizer(data['q2'])['input_ids'][:200],\n",
        "                        skip_special_tokens=True,\n",
        "                        clean_up_tokenization_spaces=True)\n",
        "  if is_eval: prompt = get_rqe_prompt(q1, q2)\n",
        "  else: prompt = get_rqe_prompt(q1, q2, data['entailment'])\n",
        "  return prompt"
      ],
      "metadata": {
        "id": "BBIGuWGWC0V4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FIiJpbnO5EF-"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class ScriptArguments:\n",
        "    # ##########################################################################\n",
        "    #                             Configuration\n",
        "    # ##########################################################################\n",
        "    model_name: Optional[str] = field(\n",
        "        # default = \"mahdii1376/ReQuEST\",\n",
        "        default = f\"/content/drive/MyDrive/llama-2-7b-chat-hf\",\n",
        "        metadata = {\"help\": \"The model that you want to train from the Hugging Face hub.\"}\n",
        "      )\n",
        "    adapter_name: Optional[str] = field(\n",
        "        default = \"LLama-RQE-Wo\",\n",
        "        metadata = {\"help\": \"The adapter name saved in the HuggingFace hub.\"}\n",
        "      )\n",
        "    save_to: Optional[str] = field(\n",
        "        default = \"Drive\",                                                       # Save to \"Hub\", or \"Drive\", or \"Both\"\n",
        "        metadata = {\"help\": \"Determine where to save Adapters\"}\n",
        "      )\n",
        "    # ##########################################################################\n",
        "    #                         Logs and Checkpoints\n",
        "    # ##########################################################################\n",
        "    logging_steps: Optional[int] = field(\n",
        "        default = 1,\n",
        "        metadata = {\"help\": \"log every X update steps\"}\n",
        "      )\n",
        "    output_dir: Optional[str] = field(\n",
        "        default = \"/content/SE-PQA\",\n",
        "        metadata = {\"help\": \"the output directory\"}\n",
        "      )\n",
        "    every_n_epochs : Optional[int] = field(\n",
        "        default = 1,\n",
        "        metadata = {\"help\": \"Save checkpoints every X epochs\"}\n",
        "      )\n",
        "    save_on_train_epoch_end: Optional[bool] = field(\n",
        "        default = None,\n",
        "        metadata = {\"help\": \"Whether to run checkpointing at the end of training epochs or validation\"}\n",
        "      )\n",
        "    total_num_samples: Optional[str] = field(\n",
        "        default = 'All',\n",
        "        metadata = {\"help\": \"Number of samples to be selected from the whole dataset\"}\n",
        "      )\n",
        "    # ##########################################################################\n",
        "    #                             Hyper-parameters\n",
        "    # ##########################################################################\n",
        "    max_epochs: Optional[int] = field(\n",
        "        default = 5,\n",
        "        metadata = {\"help\": \"maximum number of training epochs.\"}\n",
        "      )\n",
        "    learning_rate: Optional[float] = field(\n",
        "        default = 3e-5, #2e-4,\n",
        "        metadata = {\"help\": \"the learning rate\"}\n",
        "      )\n",
        "    gradient_accumulation_steps: Optional[int] = field(\n",
        "        default = 8,\n",
        "        metadata = {\"help\": \"the number of gradient accumulation steps\"}\n",
        "      )\n",
        "    gradient_checkpointing: Optional[bool] = field(\n",
        "        default = True,\n",
        "        metadata = {\"help\": \"Enables gradient checkpointing.\"}\n",
        "      )\n",
        "    per_device_train_batch_size: Optional[int] = field(\n",
        "        default = 4,\n",
        "        metadata = {\"help\": \"batch_size of training (per device)\"}\n",
        "      )\n",
        "    per_device_eval_batch_size: Optional[int] = field(\n",
        "        default = 4,\n",
        "        metadata = {\"help\": \"batch_size of validation (per device)\"}\n",
        "      )\n",
        "    max_seq_length: Optional[int] = field(\n",
        "        default = 650, #750, #650\n",
        "        metadata = {\"help\": \"maximum input sequence length\"}\n",
        "      )\n",
        "    trust_remote_code: Optional[bool] = field(\n",
        "        default = True,\n",
        "        metadata = {\"help\": '''Enable `trust_remote_code` so that it\n",
        "        will execute code present on the Hub on your local machine'''}\n",
        "      )\n",
        "    split_ratio: Optional[float] = field(\n",
        "        default = (0.8, 0.2, 0),\n",
        "        metadata = {\"help\": \"train/test/validation splits\"}\n",
        "      )\n",
        "    precision: Optional[int] = field(\n",
        "        default = 16,\n",
        "        metadata = {\"help\": \"train with 16/32/bf16 precision.\"}\n",
        "      )\n",
        "    num_sanity_val_steps: Optional[float] = field(\n",
        "        default = 0,\n",
        "        metadata = {\"help\": \"number of validation batches before the first training epoch\"}\n",
        "      )\n",
        "    max_new_tokens: Optional[int] = field(\n",
        "        default = 1,\n",
        "        metadata = {\"help\": \"the maximum number of new tokens in the generated sequences (test step)\"}\n",
        "      )\n",
        "    # ##########################################################################\n",
        "    #                             Lora Configuration\n",
        "    # ##########################################################################\n",
        "    use_peft: Optional[bool] = field(\n",
        "        default = True,\n",
        "        metadata = {\"help\": \"Wether to use PEFT or not to train adapters\"}\n",
        "      )\n",
        "    lora_r: Optional[int] = field(\n",
        "        default = 64, #16, #64, #32,\n",
        "        metadata = {\"help\": \"the r parameter of the LoRA adapters\"}\n",
        "      )\n",
        "    lora_alpha: Optional[int] = field(\n",
        "        default = 16, #16, #64, #64, #16\n",
        "        metadata = {\"help\": \"the alpha parameter of the LoRA adapters\"}\n",
        "      )\n",
        "    lora_dropout: Optional[int] = field(\n",
        "        default = 0.3,\n",
        "        metadata = {\"help\": \"the dropout rate of the LoRA adapters\"}\n",
        "      )\n",
        "    # ##########################################################################\n",
        "    #                                 BitsAndBytes\n",
        "    # ##########################################################################\n",
        "    load_in_8bit: Optional[bool] = field(\n",
        "        default = False,\n",
        "        metadata = {\"help\": \"load the model in 8 bits precision\"}\n",
        "      )\n",
        "    load_in_4bit: Optional[bool] = field(\n",
        "        default = False,\n",
        "        metadata = {\"help\": \"load the model in 4 bits precision\"}\n",
        "      )\n",
        "    use_nested_quant: Optional[bool] = field(\n",
        "        default = False,\n",
        "        metadata = {\"help\": \"Activate nested quantization for 4bit base models\"}\n",
        "      )\n",
        "    bnb_4bit_compute_dtype: Optional[str] = field(\n",
        "        default = \"float16\",\n",
        "        metadata = {\"help\": \"Compute dtype for 4bit base models\"}\n",
        "      )\n",
        "    bnb_4bit_quant_type: Optional[str] = field(\n",
        "        default = \"nf4\",\n",
        "        metadata = {\"help\": \"Quantization type fp4 or nf4\"}\n",
        "      )\n",
        "\n",
        "parser = HfArgumentParser(ScriptArguments)\n",
        "script_args = parser.parse_args_into_dataclasses(return_remaining_strings=True)[0]\n",
        "pl.seed_everything(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yBKwuzno5EGA"
      },
      "outputs": [],
      "source": [
        "class OverrideEpochStepCallback(Callback):\n",
        "    def __init__(self) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "    def on_train_epoch_end(self, trainer: pl.Trainer, pl_module: pl.LightningModule):\n",
        "        self._log_step_as_current_epoch(trainer, pl_module)\n",
        "\n",
        "    def on_test_epoch_end(self, trainer: pl.Trainer, pl_module: pl.LightningModule):\n",
        "        self._log_step_as_current_epoch(trainer, pl_module)\n",
        "\n",
        "    def on_validation_epoch_end(self, trainer: pl.Trainer, pl_module: pl.LightningModule):\n",
        "        self._log_step_as_current_epoch(trainer, pl_module)\n",
        "\n",
        "    def _log_step_as_current_epoch(self, trainer: pl.Trainer, pl_module: pl.LightningModule):\n",
        "        pl_module.log(\"step\", trainer.current_epoch + 1)\n",
        "\n",
        "checkpoint_callback = ModelCheckpoint(every_n_epochs = script_args.every_n_epochs,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mIAxFSrq5EGB"
      },
      "outputs": [],
      "source": [
        "class RQEModel(pl.LightningModule):\n",
        "    def __init__(self, script_args):\n",
        "        super(RQEModel, self).__init__()\n",
        "        self.save_hyperparameters()\n",
        "        self.Setup(script_args)\n",
        "        self.rouge = ROUGEScore()\n",
        "        self.adapter_name = script_args.adapter_name\n",
        "        self.epoch_n = 1\n",
        "        self.validation_losses = []\n",
        "        self.acc = script_args.gradient_accumulation_steps\n",
        "\n",
        "    def Setup(self, script_args):\n",
        "        if script_args.load_in_4bit and script_args.load_in_8bit:\n",
        "          raise ValueError(\"You can't load the model in 8 bits and 4 bits at the same time\")\n",
        "        elif script_args.load_in_4bit:\n",
        "          compute_dtype = getattr(torch, script_args.bnb_4bit_compute_dtype)\n",
        "\n",
        "          bnb_config = BitsAndBytesConfig(\n",
        "              load_in_4bit = script_args.load_in_4bit,\n",
        "              bnb_4bit_quant_type = script_args.bnb_4bit_quant_type,\n",
        "              bnb_4bit_compute_dtype = compute_dtype,\n",
        "              bnb_4bit_use_double_quant = script_args.use_nested_quant,\n",
        "          )\n",
        "          self.model = AutoModelForCausalLM.from_pretrained(\n",
        "              script_args.model_name,\n",
        "              quantization_config = bnb_config,\n",
        "              device_map = {\"\": 0},\n",
        "          )\n",
        "        elif script_args.load_in_8bit:\n",
        "          self.model = AutoModelForCausalLM.from_pretrained(\n",
        "              script_args.model_name,\n",
        "              load_in_8bit = True,\n",
        "              torch_dtype = torch.float16,\n",
        "              device_map = {\"\": 0},\n",
        "          )\n",
        "          self.model = prepare_model_for_kbit_training(self.model)\n",
        "\n",
        "        else:\n",
        "          self.model = AutoModelForCausalLM.from_pretrained(\n",
        "              script_args.model_name,\n",
        "              torch_dtype = torch.bfloat16,\n",
        "              device_map = {\"\": 0},\n",
        "          )\n",
        "\n",
        "        if script_args.use_peft:\n",
        "            lora_config = LoraConfig(\n",
        "                task_type = TaskType.CAUSAL_LM,\n",
        "                r = script_args.lora_r,\n",
        "                lora_alpha = script_args.lora_alpha,\n",
        "                lora_dropout = script_args.lora_dropout,\n",
        "                bias = \"none\",\n",
        "                init_lora_weights = \"pissa\",\n",
        "            )\n",
        "\n",
        "\n",
        "            self.model.print_trainable_parameters()\n",
        "\n",
        "        self.model.config.use_cache = False\n",
        "\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
        "            script_args.model_name,\n",
        "            padding_side='left'\n",
        "        )\n",
        "        self.tokenizer.pad_token_id = 0\n",
        "        self.model.config.pad_token_id = self.tokenizer.pad_token_id\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels=None):\n",
        "        output = self.model(input_ids,\n",
        "                            attention_mask=attention_mask,\n",
        "                            labels=labels\n",
        "                            )\n",
        "        return output.loss, output.logits\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        input_ids = batch['input_ids'].cuda()\n",
        "        attention_mask = batch['attention_mask'].cuda()\n",
        "        labels = batch['labels'].cuda()\n",
        "\n",
        "        loss, _ = self.forward(input_ids, attention_mask, labels)\n",
        "        self.log('train_loss', loss.item(), on_epoch=True, on_step=True)\n",
        "\n",
        "        # Log learning rate\n",
        "        optimizer = self.trainer.optimizers[0]\n",
        "        current_lr = optimizer.param_groups[0]['lr']\n",
        "        self.log('learning_rate', current_lr, on_step=True, on_epoch=True)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "      with torch.no_grad():\n",
        "        input_ids = batch['input_ids'].cuda()\n",
        "        attention_mask = batch['attention_mask'].cuda()\n",
        "        labels = batch['labels'].cuda()\n",
        "\n",
        "        val_loss, _ = self.forward(input_ids, attention_mask, labels)\n",
        "        self.log('val_loss', val_loss.item(), on_epoch=True, on_step=True)\n",
        "\n",
        "    def on_train_batch_end(self, outputs, batch, batch_idx):\n",
        "      if (self.epoch_n % 1000 == 0):\n",
        "          out_dir = f\"/content/drive/MyDrive/SE-PQA/LLAMA-RQE-WoUM/\"\n",
        "          self.model.save_pretrained(out_dir + self.adapter_name + str(int(self.epoch_n)))\n",
        "      self.epoch_n += 1\n",
        "\n",
        "    def on_validation_epoch_end(self):\n",
        "        if self.validation_losses:\n",
        "            avg_val_loss = sum(self.validation_losses) / len(self.validation_losses)\n",
        "            self.log(\"val_loss_step\", avg_val_loss)\n",
        "            self.validation_losses = []\n",
        "\n",
        "    def generate(self, *args, **kwargs):\n",
        "      return self.model.generate(*args, **kwargs)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        # Define weight decay\n",
        "        optimizer = torch.optim.AdamW(\n",
        "            self.model.parameters(),\n",
        "            lr=script_args.learning_rate,\n",
        "            weight_decay=0.2\n",
        "            )\n",
        "\n",
        "        # Define the scheduler\n",
        "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "            optimizer,\n",
        "            T_max=250 *script_args.max_epochs,              # Number of epochs before the LR reaches its minimum\n",
        "            eta_min=1e-5                                    # Minimum learning rate\n",
        "        )\n",
        "        return {\n",
        "            \"optimizer\": optimizer,\n",
        "            \"lr_scheduler\": {\n",
        "                \"scheduler\": scheduler,\n",
        "                \"interval\": \"step\",   # Scheduler steps after every epoch\n",
        "                \"frequency\": 1        # Frequency of applying the scheduler\n",
        "            }\n",
        "        }\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return self.trainer.datamodule.val_dataloader()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MyData_LLama_filepath = f\"/content/drive/MyDrive/SE-PQA/SE_PQA_Data_10000_cleaned_Len500_T20_UK.pkl\"\n",
        "MyData_LLama = pd.read_pickle(MyData_LLama_filepath)\n",
        "MyData_LLama = MyData_LLama[['body_Q1', 'body_Q2', 'entailment', 'U_Background_kn']]\n",
        "MyData_LLama = MyData_LLama.rename(columns={\n",
        "    'body_Q1': 'q1',\n",
        "    'body_Q2': 'q2'\n",
        "})\n",
        "display(MyData_LLama)\n"
      ],
      "metadata": {
        "id": "LvUB6GcHDu_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MyData_LLama[\"q1\"] = MyData_LLama[\"q1\"].str.replace(r\"^\\s*Possible Duplicates?:\\s+.*?\\s{2,}.*?\\s{2,}\", \"\", regex=True)\n",
        "MyData_LLama[\"q2\"] = MyData_LLama[\"q2\"].str.replace(r\"^\\s*Possible Duplicates?:\\s+.*?\\s{2,}.*?\\s{2,}\", \"\", regex=True)\n",
        "\n",
        "MyData_LLama[\"q1\"] = MyData_LLama[\"q1\"].str.replace(r\"^\\s*Possible Duplicates?:\\s+.*?\\s{2,}\", \"\", regex=True)\n",
        "MyData_LLama[\"q2\"] = MyData_LLama[\"q2\"].str.replace(r\"^\\s*Possible Duplicates?:\\s+.*?\\s{2,}\", \"\", regex=True)\n",
        "\n",
        "\n",
        "MyData_LLama[\"q1\"] = MyData_LLama[\"q1\"].str.replace(\"C#\", \"C\", regex=False)\n",
        "MyData_LLama[\"q1\"] = MyData_LLama[\"q1\"].str.replace(r\"#\", \"\", regex=True)\n",
        "MyData_LLama[\"q1\"] = MyData_LLama[\"q1\"].str.replace(r\"\\n\", \" \", regex=True)\n",
        "MyData_LLama[\"q2\"] = MyData_LLama[\"q2\"].str.replace(\"C#\", \"C\", regex=False)\n",
        "MyData_LLama[\"q2\"] = MyData_LLama[\"q2\"].str.replace(r\"#\", \"\", regex=True)\n",
        "MyData_LLama[\"q2\"] = MyData_LLama[\"q2\"].str.replace(r\"\\n\", \" \", regex=True)"
      ],
      "metadata": {
        "id": "GhLRQQN9EotZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RQEDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data, tokenizer, max_len, is_eval):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "        self.is_eval = is_eval\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "      row_data = self.data.iloc[index]\n",
        "      prompt = generate_prompt_rqe(row_data, self.tokenizer, self.is_eval)\n",
        "      prompt_encoding = self.tokenizer(prompt,\n",
        "                                       max_length = self.max_len,\n",
        "                                       padding = 'max_length',\n",
        "                                       truncation = True,\n",
        "                                       add_special_tokens = True,\n",
        "                                       return_tensors = 'pt',\n",
        "                                       )\n",
        "      input_ids = prompt_encoding['input_ids'].squeeze()\n",
        "      attention_mask = prompt_encoding['attention_mask'].squeeze()\n",
        "\n",
        "      if self.is_eval == False:\n",
        "        response_index = get_response_index(input_ids, 'RQE')\n",
        "        if response_index:\n",
        "          labels = torch.cat((torch.full((response_index,), -100), input_ids[response_index:])).squeeze()\n",
        "        else:\n",
        "          print('response_index not found')\n",
        "      else:\n",
        "        labels = self.tokenizer(row_data['entailment'],\n",
        "                                add_special_tokens = False,\n",
        "                                truncation = True,\n",
        "                                max_length = 1,\n",
        "                                padding = 'max_length',\n",
        "                                return_tensors='pt',\n",
        "                                )\n",
        "        labels = labels['input_ids'].squeeze()\n",
        "      return {\n",
        "          'input_ids': input_ids,\n",
        "          'attention_mask': attention_mask,\n",
        "          'labels': labels\n",
        "          }"
      ],
      "metadata": {
        "id": "-MbepUEiE3Ct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RQEDataModule(pl.LightningDataModule):\n",
        "    def __init__(self, data, tokenizer, script_args):\n",
        "        super().__init__()\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.per_device_train_batch_size = script_args.per_device_train_batch_size\n",
        "        self.per_device_eval_batch_size = script_args.per_device_eval_batch_size\n",
        "        self.max_len = script_args.max_seq_length\n",
        "        self.setup()\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        len_tr = int(script_args.split_ratio[0] * self.data.shape[0])\n",
        "        len_te = int(script_args.split_ratio[1] * self.data.shape[0])\n",
        "        train_data, test_data = train_test_split(self.data,\n",
        "                                                 train_size=len_tr,\n",
        "                                                 shuffle=False,\n",
        "                                                #  random_state=42\n",
        "                                                 )\n",
        "\n",
        "        train_data.reset_index(drop=True, inplace=True)\n",
        "        test_data.reset_index(drop=True, inplace=True)\n",
        "\n",
        "        self.train_data = RQEDataset(train_data, self.tokenizer, self.max_len, is_eval=False)\n",
        "        self.test_data = RQEDataset(test_data, self.tokenizer, self.max_len, is_eval=True)\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return torch.utils.data.DataLoader(\n",
        "            self.train_data,\n",
        "            batch_size=self.per_device_train_batch_size,\n",
        "            shuffle=True,\n",
        "            num_workers=4,\n",
        "        )\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return torch.utils.data.DataLoader(\n",
        "            self.test_data,\n",
        "            sampler = torch.utils.data.SequentialSampler(self.test_data,),\n",
        "            batch_size= self.per_device_eval_batch_size,\n",
        "            num_workers=4\n",
        "        )"
      ],
      "metadata": {
        "id": "3ItRhtFtE7RW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MyModel = RQEModel(script_args)\n",
        "logger = TensorBoardLogger(script_args.output_dir + 'logs', name=\"RQE\")\n",
        "\n",
        "print(MyModel)\n",
        "print(\"#\"*60, \"\\n\\t\\t\\t Model Configuration\\n\", \"#\"*60)\n",
        "print(MyModel.model.config)"
      ],
      "metadata": {
        "id": "IfS6zs-yFnLS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XBDznb6FFeT-"
      },
      "outputs": [],
      "source": [
        "MyModel.tokenizer.truncation_side = 'left'\n",
        "DataModule = RQEDataModule(\n",
        "    MyData_LLama,\n",
        "    MyModel.tokenizer,\n",
        "    script_args\n",
        ")\n",
        "print(\"num train batches\", len(DataModule.train_dataloader()))\n",
        "print(\"num test batches\", len(DataModule.test_dataloader()))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for sample in DataModule.train_dataloader():\n",
        "  print(MyModel.tokenizer.batch_decode(sample['input_ids'])[0])\n",
        "  print(MyModel.tokenizer.batch_decode(sample['attention_mask'])[0])\n",
        "  print(sample['labels'][0].tolist())\n",
        "  break"
      ],
      "metadata": {
        "id": "Bznvs-FKFeT_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for sample in DataModule.test_dataloader():\n",
        "  print(sample)\n",
        "  print(MyModel.tokenizer.batch_decode(sample['labels']))\n",
        "  break"
      ],
      "metadata": {
        "id": "wfLAAowFFeUA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GVbOwUaZFeUA"
      },
      "outputs": [],
      "source": [
        "trainer = pl.Trainer(\n",
        "    logger = logger,\n",
        "    log_every_n_steps = script_args.logging_steps,\n",
        "    max_epochs = script_args.max_epochs,\n",
        "    accumulate_grad_batches = script_args.gradient_accumulation_steps,\n",
        "    num_sanity_val_steps = script_args.num_sanity_val_steps,\n",
        "    callbacks = [OverrideEpochStepCallback(), checkpoint_callback],                                  #\n",
        "    default_root_dir= script_args.output_dir + 'Checkpoints',\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m5rt2XJrFeUB"
      },
      "outputs": [],
      "source": [
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir /content/SE-PQAlogs\n",
        "\n",
        "trainer.fit(\n",
        "    MyModel,\n",
        "    datamodule=DataModule,\n",
        "    # ckpt_path = \"/content/ReQuESTlogs/TG/version_0/checkpoints/epoch=0-step=1.ckpt\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r /content/SE-PQAlogs /content/drive/MyDrive/SE-PQA/SE-PQAlogs_RQE_WoUM"
      ],
      "metadata": {
        "id": "vm_GVCHaFeUC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir /content/SE-PQAlogs/RQE"
      ],
      "metadata": {
        "id": "JVANhf7egXbe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import gc\n",
        "# tokenizer=None\n",
        "trainer=None\n",
        "MyModel = None\n",
        "# fModel = None\n",
        "# BaseModel = None\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "71kgt-yvgdaY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BaseModel= AutoModelForCausalLM.from_pretrained(\n",
        "    f\"/content/drive/MyDrive/llama-2-7b-chat-hf\",\n",
        "    device_map={\"\": 0},\n",
        "    offload_folder=\"offload\",\n",
        "    offload_state_dict = True,\n",
        "    # load_in_8bit = True\n",
        "    )"
      ],
      "metadata": {
        "id": "aa_Tw7M4gl1N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "address = f\"/content/drive/MyDrive/SE-PQA/LLAMA-RQE-WoUM/LLama-RQE-Wo10000\"\n",
        "print(\"\\n Loading model from \", address, \"\\n\")\n",
        "config = PeftConfig.from_pretrained(address)\n",
        "fModel= PeftModel.from_pretrained(BaseModel, address, device_map={\"\": 0})\n",
        "fModel = fModel.merge_and_unload()"
      ],
      "metadata": {
        "id": "iYTi3mh3govW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    f'/content/drive/MyDrive/llama-2-7b-chat-hf',\n",
        "    padding_side='left'\n",
        "    )\n",
        "tokenizer.pad_token_id = 0\n",
        "\n",
        "fModel.config.pad_token_id = tokenizer.pad_token_id\n",
        "fModel.config.mask_token_id = tokenizer.mask_token_id\n",
        "print(fModel)\n",
        "print(fModel.config)\n",
        "print(\"\\n Model successfully loded from \", address, \"\\n\")"
      ],
      "metadata": {
        "id": "kbgY2dbIg3OZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "address3 = f\"/content/drive/MyDrive/SE-PQA/retrival_results_mpnet-50.pkl\"\n",
        "retrival_results = pd.read_pickle(address3)\n",
        "\n",
        "def clean_text_list(text_list):\n",
        "    combined_text = '###'.join(text_list)\n",
        "    cleaned_text = re.sub(r'\\s+', ' ', combined_text).strip()\n",
        "    return cleaned_text.split('###')\n",
        "\n",
        "retrival_results['Top n Similar Questions Body'] = retrival_results['Top n Similar Questions Body'].apply(clean_text_list)"
      ],
      "metadata": {
        "id": "sYD_c8Oihu-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(retrival_results)"
      ],
      "metadata": {
        "id": "o8YKg4l3iEyh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MyData3 = pd.read_pickle(\"/content/drive/MyDrive/SE-PQA/SE_PQA_Data_10000_cleaned_Len500_T20_UK.pkl\")\n",
        "len_tr = int(0.8 * MyData3.shape[0])\n",
        "len_te = int(0.2 * MyData3.shape[0])\n",
        "train_data, test_data = train_test_split(MyData3,\n",
        "                                          test_size=len_te,\n",
        "                                          random_state=42)\n",
        "test_data\n",
        "test_data = test_data.sample(n=148, random_state=42)\n",
        "test_data.head(3)"
      ],
      "metadata": {
        "id": "-gHDSdfjihRI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m=50\n",
        "test_data = test_data[['body_Q1', 'U_Background_kn']]\n",
        "RQETestDataNew = test_data.loc[test_data.index.repeat(m)].reset_index(drop=True)\n",
        "similar_questions = [item for sublist in retrival_results['Top n Similar Questions Body'].values for item in sublist[:m]]\n",
        "similar_answers = [item for sublist in retrival_results['Top n Candidate Answers'].values for item in sublist[:m]]\n",
        "\n",
        "RQETestDataNew['q2'] = similar_questions\n",
        "RQETestDataNew['CandidateAnswerBody'] = similar_answers\n",
        "RQETestDataNew['entailment'] = ''\n",
        "\n",
        "display(RQETestDataNew.head(m+1))\n",
        "display(f'Number of samples in new RQE test data = ' + str(len(RQETestDataNew)) + '</b>')"
      ],
      "metadata": {
        "id": "yfMHE1P2h3eZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CosineScores = [item for sublist in retrival_results['Cosine similarities'].values for item in sublist[:m]]\n",
        "q2_ids = [item for sublist in retrival_results['Top n Similar Questions (id)'].values for item in sublist[:m]]\n",
        "RQETestDataNew['CosineSimilarities'] = CosineScores\n",
        "RQETestDataNew['CandidateQuestionID'] = q2_ids\n",
        "\n",
        "display(RQETestDataNew.head(5))\n",
        "display(RQETestDataNew.shape)"
      ],
      "metadata": {
        "id": "K_iPa-F6jfAo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = train_data[['body_Q1', 'body_Q2', 'entailment', 'U_Background_kn']].copy()\n",
        "display(train_data.head(5))\n",
        "test_data = RQETestDataNew[['body_Q1', 'q2', 'entailment', 'U_Background_kn']].copy()\n",
        "display(test_data.head(5))\n",
        "\n",
        "train_data.rename(columns={'body_Q1': 'q1', 'body_Q2':'q2'}, inplace=True)\n",
        "test_data.rename(columns={'body_Q1': 'q1'}, inplace=True)\n",
        "MyData = pd.concat([train_data, test_data]).reset_index()\n",
        "display(MyData)"
      ],
      "metadata": {
        "id": "4xB2GhLRkggs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "address6 = f\"/content/drive/MyDrive/SE-PQA/MyQAData_MPNet50.pkl\"\n",
        "MyData.to_pickle(address6)\n",
        "MyData = pd.read_pickle(address6)"
      ],
      "metadata": {
        "id": "LH_Bc3zVk8BV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RQEDataModule(pl.LightningDataModule):\n",
        "    def __init__(self, data, tokenizer, script_args):\n",
        "        super().__init__()\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.per_device_train_batch_size = script_args.per_device_train_batch_size\n",
        "        self.per_device_eval_batch_size = script_args.per_device_eval_batch_size\n",
        "        self.max_len = script_args.max_seq_length\n",
        "        self.setup()\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        len_tr = 8000\n",
        "        len_te = 7400\n",
        "        train_data, test_data = train_test_split(self.data,\n",
        "                                                 train_size=len_tr,\n",
        "                                                 shuffle=False,\n",
        "                                                #  random_state=42\n",
        "                                                 )\n",
        "\n",
        "        train_data.reset_index(drop=True, inplace=True)\n",
        "        test_data.reset_index(drop=True, inplace=True)\n",
        "\n",
        "        self.train_data = RQEDataset(train_data, self.tokenizer, self.max_len, is_eval=False)\n",
        "        self.test_data = RQEDataset(test_data, self.tokenizer, self.max_len, is_eval=True)\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return torch.utils.data.DataLoader(\n",
        "            self.train_data,\n",
        "            batch_size=self.per_device_train_batch_size,\n",
        "            shuffle=True,\n",
        "            num_workers=4,\n",
        "        )\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return torch.utils.data.DataLoader(\n",
        "            self.test_data,\n",
        "            sampler = torch.utils.data.SequentialSampler(self.test_data,),\n",
        "            batch_size= self.per_device_eval_batch_size,\n",
        "            num_workers=4\n",
        "        )"
      ],
      "metadata": {
        "id": "koTypEIxlNKl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Data_RQE = RQEDataModule(\n",
        "    MyData,\n",
        "    tokenizer,\n",
        "    script_args\n",
        ")\n",
        "print(\"num train batches\", len(Data_RQE.train_dataloader()))\n",
        "print(\"num test batches\", len(Data_RQE.test_dataloader()))"
      ],
      "metadata": {
        "id": "_DB1EhQklJR7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in Data_RQE.test_dataloader():\n",
        "  print(tokenizer.decode(i['input_ids'][0]))\n",
        "  # print(tokenizer.batch_decode(i['input_ids']))\n",
        "  print(tokenizer.batch_decode(i['labels']))\n",
        "  break"
      ],
      "metadata": {
        "id": "vw5vKA3vlVD9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fModel.eval()\n",
        "results = []\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in Data_RQE.test_dataloader():\n",
        "        input_ids = batch['input_ids'].cuda()\n",
        "        attention_mask = batch['attention_mask'].cuda()\n",
        "\n",
        "        generated_txts_ids = fModel.generate(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            max_new_tokens=script_args.max_new_tokens,\n",
        "            do_sample=False,\n",
        "            temperature=0.0000001,\n",
        "        )\n",
        "\n",
        "        for i in range(input_ids.size(0)):\n",
        "            single_generated_ids = generated_txts_ids[i]\n",
        "\n",
        "            response_start_idx = get_response_index(single_generated_ids, 'RQE')\n",
        "            single_generated_txt = tokenizer.decode(\n",
        "                single_generated_ids[response_start_idx:],\n",
        "                skip_special_tokens=True,\n",
        "                clean_up_tokenization_spaces=True\n",
        "            )\n",
        "            results.append(single_generated_txt[1:])\n",
        "            # print(results)\n",
        "            # input()\n",
        "end_time = time.time()\n",
        "\n",
        "display(f\"--- {end_time - start_time} seconds ---\")\n",
        "display(f\"Number of results: {len(results)}\")"
      ],
      "metadata": {
        "id": "ac4A6gaglbBm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(f\"--- {end_time - start_time} seconds ---\")\n",
        "display(f\"Number of results: {len(results)}\")"
      ],
      "metadata": {
        "id": "ACx_dbMmmKPc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "address7 = f\"/content/drive/MyDrive/SE-PQA/results_MPNet_50_WoUM_10000.pkl\"\n",
        "pd.DataFrame(results, columns = ['predicted_label']).to_pickle(address7)\n",
        "\n",
        "test_results_df = pd.read_pickle(address7)\n",
        "print(test_results_df)"
      ],
      "metadata": {
        "id": "3xdO6Jz7l32Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Generator"
      ],
      "metadata": {
        "id": "WYtILoZooj5M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n"
      ],
      "metadata": {
        "id": "TPzvrUqJsD7m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MyData_wo = pd.read_pickle(f\"/content/drive/MyDrive/SE-PQA/results_MPNet_50_WoUM_10000.pkl\")\n",
        "MyData_w = pd.read_pickle(f\"/content/drive/MyDrive/SE-PQA/results_MPNet_50_UM_10000.pkl\")\n",
        "MyData_WithoutPreds = pd.read_pickle(f\"/content/drive/MyDrive/SE-PQA/MyQAData_MPNet50.pkl\")\n",
        "retrival_results = pd.read_pickle(f\"/content/drive/MyDrive/SE-PQA/retrival_results_mpnet-50.pkl\")"
      ],
      "metadata": {
        "id": "s0XuQ3h0no-x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MyData3 = pd.read_pickle(\"/content/drive/MyDrive/SE-PQA/SE_PQA_Data_10000_cleaned_Len500_T20_UK.pkl\")\n",
        "len_tr = int(0.8 * MyData3.shape[0])\n",
        "len_te = int(0.2 * MyData3.shape[0])\n",
        "train_data, test_data = train_test_split(MyData3,\n",
        "                                          test_size=len_te,\n",
        "                                          random_state=42)\n",
        "test_data\n",
        "test_data = test_data.sample(n=148, random_state=42)\n",
        "test_data_answers = test_data.copy()\n",
        "\n",
        "m=50\n",
        "test_data = test_data[['body_Q1', 'U_Background_kn']]\n",
        "RQETestDataNew = test_data.loc[test_data.index.repeat(m)].reset_index(drop=True)\n",
        "similar_questions = [item for sublist in retrival_results['Top n Similar Questions Body'].values for item in sublist[:m]]\n",
        "similar_answers = [item for sublist in retrival_results['Top n Candidate Answers'].values for item in sublist[:m]]\n",
        "\n",
        "RQETestDataNew['q2'] = similar_questions\n",
        "RQETestDataNew['CandidateAnswerBody'] = similar_answers\n",
        "RQETestDataNew['entailment'] = ''\n",
        "\n",
        "display(RQETestDataNew.head(m+1))\n",
        "display(f'Number of samples in new RQE test data = ' + str(len(RQETestDataNew)) + '</b>')\n",
        "\n",
        "CosineScores = [item for sublist in retrival_results['Cosine similarities'].values for item in sublist[:m]]\n",
        "q2_ids = [item for sublist in retrival_results['Top n Similar Questions (id)'].values for item in sublist[:m]]\n",
        "RQETestDataNew['CosineSimilarities'] = CosineScores\n",
        "RQETestDataNew['CandidateQuestionID'] = q2_ids\n",
        "\n",
        "display(RQETestDataNew.head(5))\n",
        "display(RQETestDataNew.shape)"
      ],
      "metadata": {
        "id": "fh_32Pqbs08Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MyData_WithoutPreds = MyData_WithoutPreds[8000:]"
      ],
      "metadata": {
        "id": "N_ZE8KyssScC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RQE6_vs_RQE7 = pd.concat([MyData_wo, MyData_w], axis = 1)\n",
        "\n",
        "RQE6_vs_RQE7.columns = ['Without_UM', 'With_UM']\n",
        "print('Do models perform the same: ', MyData_wo.equals(MyData_w), \"\\n\")                # Two models did not perform equally\n",
        "display(RQE6_vs_RQE7)\n",
        "display(RQE6_vs_RQE7.shape)"
      ],
      "metadata": {
        "id": "i-BW7WaQoqFK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MyData_WithPreds = pd.concat([RQETestDataNew.reset_index(drop=True), RQE6_vs_RQE7], axis=1)\n",
        "MyData_WithPreds.shape"
      ],
      "metadata": {
        "id": "wPUGvmuBo3Yf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MyData_WithPreds.head(3)"
      ],
      "metadata": {
        "id": "kchA5MkSsmxa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MyData_WithPreds = MyData_WithPreds.groupby(\n",
        "    ['body_Q1'],\n",
        "    group_keys=False\n",
        "    ).apply(lambda x: x.sort_values(by='CosineSimilarities', ascending=False))\n",
        "\n",
        "display(MyData_WithPreds.head(5))\n",
        "print(MyData_WithPreds.shape)"
      ],
      "metadata": {
        "id": "-aoAvH8KpBnv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answer_mapping = test_data_answers.groupby('body_Q1')['answer_body_Q1'].first()  # or .last()\n",
        "MyData_WithPreds['AcceptedAnswer'] = MyData_WithPreds['body_Q1'].map(answer_mapping)"
      ],
      "metadata": {
        "id": "k1PxxAzZ9T12"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MyData_WithPreds[(MyData_WithPreds[\"With_UM\"]!='positive') & (MyData_WithPreds[\"With_UM\"]!='negative')].head(5)"
      ],
      "metadata": {
        "id": "y_I1_EyY9vlq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MyData_WithPreds = MyData_WithPreds.drop_duplicates(subset=['body_Q1', 'q2'], keep='last')\n",
        "MyData_WithPreds"
      ],
      "metadata": {
        "id": "Ii-YhcLvvESM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f = 3\n",
        "entailed_df_Without_UM = MyData_WithPreds[MyData_WithPreds['Without_UM'] == 'positive']\n",
        "top3_entailed_Without_UM = entailed_df_Without_UM.groupby(['body_Q1']).head(f)\n",
        "display(top3_entailed_Without_UM.shape)\n",
        "display(top3_entailed_Without_UM.head(2*f))\n",
        "\n",
        "entailed_df_With_UM = MyData_WithPreds[MyData_WithPreds['With_UM'] == 'positive']\n",
        "top3_entailed_With_UM = entailed_df_With_UM.groupby(['body_Q1']).head(f)\n",
        "display(top3_entailed_With_UM.shape)\n",
        "display(top3_entailed_With_UM.head(2*f))"
      ],
      "metadata": {
        "id": "5uucEcUKpF9F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zX7L7Q9ZeBrE"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import re\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W17hRABzf7KF"
      },
      "outputs": [],
      "source": [
        "!pip install -q rouge-score bert-score nltk transformers torch\n",
        "!pip install -q sacrebleu\n",
        "!pip install -q sentence-transformers\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "import sacrebleu\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from scipy.spatial.distance import cosine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZB68PGjCgeTR"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import torch\n",
        "import bert_score\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from rouge_score import rouge_scorer\n",
        "from nltk.translate.meteor_score import meteor_score\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tyffr27RX1cZ"
      },
      "outputs": [],
      "source": [
        "!pip install -q unbabel-comet\n",
        "\n",
        "from comet import download_model, load_from_checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZZ9e8SYGfHGC"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## LLAMA2\n",
        "import torch\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from langchain import LLMChain, HuggingFacePipeline, PromptTemplate\n"
      ],
      "metadata": {
        "id": "NCyPfKkIyCCI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZPQEOprZa97z"
      },
      "outputs": [],
      "source": [
        "!pip install -q langchain-community langchain-core\n",
        "!pip install -q --upgrade langchain\n",
        "\n",
        "from langchain import PromptTemplate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Ei5-uL4zH1T"
      },
      "outputs": [],
      "source": [
        "#\n",
        "\n",
        "template_WoUM = \"\"\"********************{i}********************\n",
        "You are an AI language model tasked with synthesizing an accurate, complete, and well-structured answer based solely on the provided expert-written answers. Follow these strict guidelines:\n",
        "Analyze each provided answer carefully.\n",
        "Extract relevant words, phrases, sentences, or subsequences that contribute to answering the given question.\n",
        "Synthesize a comprehensive, continuous, and well-structured response without any headings, subheadings, or explicit references to the original answers (e.g., do not say \"as stated in Answer 1\").\n",
        "Rephrase extracted content where necessary to align with the exact requirements of the question. If an answer provides a solution to a slightly different but related problem, adapt the phrasing while preserving factual accuracy.\n",
        "Incorporate all relevant information from the answers, ensuring that multiple valid solutions, perspectives, or explanations are included where applicable. No relevant information should be omitted.\n",
        "Remove non-relevant parts that do not contribute to answering the question.\n",
        "Do not introduce any external information beyond what is contained in the provided answers. If an answer is not covered in the provided content, do not generate additional details from external knowledge.\n",
        "If none of the provided answers sufficiently address the question, clearly state: \"The question could not be answered based on the available context.\"\n",
        "Prioritize precision over recall, ensuring that responses are accurate and directly relevant to the question. However, the answer should also be as complete as possible while maintaining clarity and conciseness.\n",
        "Now, based on these instructions, analyze the following question and answers, then generate the best possible response.\n",
        "\n",
        "### Question: {query}\n",
        "### Answer1: {A1}\n",
        "### Answer2: {A2}\n",
        "### Answer3: {A3}\n",
        "### Answer:\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "def generate_summary(i, query, a1, a2, a3):\n",
        "    prompt_temp_WoUM = PromptTemplate(\n",
        "        template=template_WoUM,\n",
        "        input_variables=[\"query\", \"A1\", \"A2\", \"A3\"]\n",
        "        )\n",
        "    summary = prompt_temp_WoUM.format(i = i, query = query, A1 = a1, A2 = a2, A3 = a3)\n",
        "\n",
        "    return summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TvZYekwEBN53"
      },
      "outputs": [],
      "source": [
        "# select wo (without user-modeling) or w (with user-modeling)\n",
        "state = 'w'\n",
        "# state = 'wo'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M9PRkzCkjrxL"
      },
      "outputs": [],
      "source": [
        "if state == 'wo':\n",
        "  display('State : without user-modeling')\n",
        "  df = top3_entailed_Without_UM.copy()\n",
        "else:\n",
        "  display('State : with user-modeling')\n",
        "  df = top3_entailed_With_UM.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GOne-YVk0Iby"
      },
      "outputs": [],
      "source": [
        "from langchain.memory import ConversationBufferMemory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1k2VRlx9gB66"
      },
      "outputs": [],
      "source": [
        "generated_answers = []\n",
        "retrieval_time = 0\n",
        "i = 0\n",
        "for name, group in df.groupby(['body_Q1']):\n",
        "    candidate_answers = group['CandidateAnswerBody'].tolist()\n",
        "    group['AcceptedAnswer'] = group['AcceptedAnswer'].str.replace(\"\\n\", \" \", regex=False)\n",
        "    Gold = group['AcceptedAnswer'].iloc[0]\n",
        "    a1 = candidate_answers[0]\n",
        "    if (len(candidate_answers)<2):\n",
        "      a2 = \"\"\n",
        "    else:\n",
        "      a2 = candidate_answers[1]\n",
        "\n",
        "    if (len(candidate_answers)<3):\n",
        "      a3 = \"\"\n",
        "    else:\n",
        "      a3 = candidate_answers[2]\n",
        "    group['body_Q1'] = group['body_Q1'].str.replace(\"\\n\", \" \", regex=False)\n",
        "    query = group['body_Q1'].iloc[0]\n",
        "    # start_time = time.time()\n",
        "\n",
        "    generated_answer = generate_summary(i, query, a1, a2, a3)\n",
        "    e = f\n",
        "    file1_path = f\"/content/drive/MyDrive/SE-PQA/NewPrompts_{state}_{f}.txt\"\n",
        "    with open(file1_path, \"a\") as f1:\n",
        "      f1.write(generated_answer)\n",
        "    f1.close()\n",
        "\n",
        "\n",
        "    file1_path = f\"/content/drive/MyDrive/SE-PQA/Answers_{state}_{f}.txt\"\n",
        "    with open(file1_path, \"a\") as f2:\n",
        "      f2.write(Gold + \"------------------------\")\n",
        "    f2.close()\n",
        "    i = i + 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Quantitave Analysis"
      ],
      "metadata": {
        "id": "Ag-PZYdeEbcM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wzEnl7-FeyL0"
      },
      "outputs": [],
      "source": [
        "def clean_text1(text):\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "def clean_text2(text):\n",
        "    text = re.sub(r\"(\\*{2,3}|`)\", \" \", text)\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"<pre>\", \"\", text)\n",
        "    text = re.sub(r\"<code>\", \"\", text)\n",
        "    text = re.sub(r'[]', \"'\", text)\n",
        "    text = re.sub(r\"[]\", \"'\", text)\n",
        "    text = re.sub(r\"[`]\", \"\", text)\n",
        "    text = re.sub(r'[^a-z0-9\\s/:.,!?\\'()*-]', ' ', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "\n",
        "def read_lines1(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        return [clean_text1(line.strip()) for line in f]\n",
        "\n",
        "\n",
        "def read_lines2(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        return [clean_text2(line.strip()) for line in f]\n",
        "\n",
        "\n",
        "def compute_rouge(references, candidates):\n",
        "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "    scores = {'rouge1': [], 'rouge2': [], 'rougeL': []}\n",
        "    for ref, cand in zip(references, candidates):\n",
        "        score = scorer.score(ref, cand)\n",
        "        for key in scores.keys():\n",
        "            scores[key].append(score[key])\n",
        "    avg_scores = {k: {\n",
        "        'precision': sum(s.precision for s in v) / len(v),\n",
        "        'recall': sum(s.recall for s in v) / len(v),\n",
        "        'f1': sum(s.fmeasure for s in v) / len(v)\n",
        "    } for k, v in scores.items()}\n",
        "    return avg_scores\n",
        "\n",
        "\n",
        "def compute_bleu(references, candidates):\n",
        "    smoothing = SmoothingFunction().method1\n",
        "    bleu_scores = {f\"bleu-{i}\": [] for i in range(1, 5)}\n",
        "    for ref, cand in zip(references, candidates):\n",
        "        ref_tokens = [ref.split()]\n",
        "        cand_tokens = cand.split()\n",
        "        for i in range(1, 5):\n",
        "            score = sentence_bleu(ref_tokens, cand_tokens, weights=[1/i]*i + [0]*(4-i), smoothing_function=smoothing)\n",
        "            bleu_scores[f\"bleu-{i}\"].append(score)\n",
        "    return {k: sum(v) / len(v) for k, v in bleu_scores.items()}\n",
        "\n",
        "\n",
        "def compute_bert_score(references, candidates):\n",
        "    P, R, F1 = bert_score.score(candidates, references, lang=\"en\")\n",
        "    return {\n",
        "        \"precision\": P.mean().item(),\n",
        "        \"recall\": R.mean().item(),\n",
        "        \"f1\": F1.mean().item()\n",
        "    }\n",
        "\n",
        "\n",
        "def compute_meteor(references, candidates):\n",
        "    scores = [meteor_score([ref.split()], cand.split()) for ref, cand in zip(references, candidates)]\n",
        "    return sum(scores) / len(scores)\n",
        "\n",
        "\n",
        "def compute_perplexity(sentences):\n",
        "    model_name = \"gpt2\"\n",
        "    model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "    model.eval()\n",
        "    perplexities = []\n",
        "    for sentence in sentences:\n",
        "        inputs = tokenizer(sentence, return_tensors=\"pt\", truncation=True, max_length=1024)\n",
        "        if len(inputs[\"input_ids\"].squeeze()) == 0:\n",
        "            continue\n",
        "        inputs[\"position_ids\"] = torch.arange(inputs[\"input_ids\"].size(1)).unsqueeze(0)\n",
        "        with torch.no_grad():\n",
        "            loss = model(**inputs, labels=inputs[\"input_ids\"]).loss\n",
        "        perplexities.append(torch.exp(loss).item())\n",
        "    return sum(perplexities) / len(perplexities) if perplexities else float(\"inf\")\n",
        "\n",
        "\n",
        "\n",
        "def compute_ter(references, candidates):\n",
        "    scores = [sacrebleu.sentence_ter(cand, [ref]).score for ref, cand in zip(references, candidates)]\n",
        "    return sum(scores) / len(scores)\n",
        "\n",
        "\n",
        "def compute_chrf(references, candidates):\n",
        "    scores = [sacrebleu.sentence_chrf(cand, [ref]).score for ref, cand in zip(references, candidates)]\n",
        "    return sum(scores) / len(scores)\n",
        "\n",
        "\n",
        "def compute_sentence_bert(references, candidates):\n",
        "    model = SentenceTransformer(\"all-mpnet-base-v2\")\n",
        "    ref_embeddings = model.encode(references, convert_to_tensor=True)\n",
        "    cand_embeddings = model.encode(candidates, convert_to_tensor=True)\n",
        "    similarities = [1 - cosine(ref_emb.cpu(), cand_emb.cpu())\n",
        "                    for ref_emb, cand_emb in zip(ref_embeddings, cand_embeddings)]\n",
        "    return sum(similarities) / len(similarities)\n",
        "\n",
        "\n",
        "def compute_comet(references, candidates):\n",
        "    model_path = download_model(\"Unbabel/wmt22-comet-da\")\n",
        "    model = load_from_checkpoint(model_path)\n",
        "    data = [{\"src\": ref, \"mt\": cand, \"ref\": ref} for ref, cand in zip(references, candidates)]\n",
        "    scores = model.predict(data, batch_size=8)\n",
        "    return sum(scores[\"scores\"]) / len(scores[\"scores\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S4SQHyMVhQFi"
      },
      "outputs": [],
      "source": [
        "def main(file1, file2):\n",
        "    references = read_lines1(file1)\n",
        "    candidates = read_lines1(file2)\n",
        "    perplexity = compute_perplexity(candidates)\n",
        "\n",
        "    references = read_lines2(file1)\n",
        "    candidates = read_lines2(file2)\n",
        "\n",
        "    rouge_scores = compute_rouge(references, candidates)\n",
        "    bleu_scores = compute_bleu(references, candidates)\n",
        "    bert_scores = compute_bert_score(references, candidates)\n",
        "    meteor = compute_meteor(references, candidates)\n",
        "    ter_score = compute_ter(references, candidates)\n",
        "    chrf_score = compute_chrf(references, candidates)\n",
        "    comet_score = compute_comet(references, candidates)\n",
        "    sbert_score = compute_sentence_bert(references, candidates)\n",
        "\n",
        "    print(f\"TER Score: {ter_score:.4f}\")\n",
        "    print(f\"chrF Score: {chrf_score:.4f}\")\n",
        "    print(f\"COMET Score: {comet_score:.4f}\")\n",
        "    print(f\"SBERT Cosine Similarity: {sbert_score:.4f}\")\n",
        "    print(\"ROUGE Scores:\", rouge_scores)\n",
        "    print(\"BLEU Scores:\", bleu_scores)\n",
        "    print(\"BERTScore:\", bert_scores)\n",
        "    print(\"METEOR Score:\", meteor)\n",
        "    print(\"Perplexity:\", perplexity)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F0hT0aHzmy5L"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "logging.getLogger(\"transformers.modeling_utils\").setLevel(logging.ERROR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nSdu8waUhTD3"
      },
      "outputs": [],
      "source": [
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    UM_path = \"/content/drive/MyDrive/SE-PQA/SE-PQA-UM-3.txt\"\n",
        "    WoUM_path = \"/content/drive/MyDrive/SE-PQA/SE-PQA-WoUM-3.txt\"\n",
        "    AccA_path_UM = \"/content/drive/MyDrive/SE-PQA/Answers_w_3.txt\"\n",
        "    AccA_path_WoUM = \"/content/drive/MyDrive/SE-PQA/Answers_wo_3.txt\"\n",
        "\n",
        "    main(AccA_path_UM, UM_path)\n",
        "    main(AccA_path_WoUM, WoUM_path)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "machine_shape": "hm",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}